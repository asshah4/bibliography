Automatically generated by Mendeley Desktop 1.19
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{Mark2016,
abstract = {Pvalues and hypothesis testing methods are frequently misused in clinical research. Much of this misuse appears to be owing to the widespread, mistaken belief that they provide simple, reliable, and objective triage tools for separating the true and important from the untrue or unimportant. The primary focus in interpreting therapeutic clinical research data should be on the treatment (“oomph”) effect, a metaphorical force that moves patients given an effective treatment to a different clinical state relative to their control counterparts. This effect is assessed using 2 complementary types of statistical measures calculated from the data, namely, effect magnitude or size and precision of the effect size. In a randomized trial, effect size is often summarized using constructs, such as odds ratios, hazard ratios, relative risks, or adverse event rate differences. How large a treatment effect has to be to be consequential is a matter for clinical judgment. The precision of the effect size (conceptually related to the amount of spread in the data) is usually addressed with confidence intervals.Pvalues (significance tests) were first proposed as an informal heuristic to help assess how “unexpected” the observed effect size was if the true state of nature was no effect or no difference. Hypothesis testing was a modification of the significance test approach that envisioned controlling the false-positive rate of study results over many (hypothetical) repetitions of the experiment of interest. Both can be helpful but, by themselves, provide only a tunnel vision perspective on study results that ignores the clinical effects the study was conducted to measure.},
author = {Mark, Daniel B. and Lee, Kerry L. and Harrell, Frank E.},
doi = {10.1001/jamacardio.2016.3312},
isbn = {2380-6591 (Electronic)},
issn = {23806591},
journal = {JAMA Cardiology},
keywords = {adverse event,clinical research,false-positive results,hypothesis testing,null hypothesis testing,p-value,statistical tests},
month = {dec},
number = {9},
pages = {1048--1054},
pmid = {27732700},
publisher = {American Medical Association},
title = {{Understanding the role of P values and hypothesis tests in clinical research}},
url = {http://cardiology.jamanetwork.com/article.aspx?doi=10.1001/jamacardio.2016.3312},
volume = {1},
year = {2016}
}
@article{Lo2015,
abstract = {Thus far, genome-wide association studies (GWAS) have been disappointing in the inability of investigators to use the results of identified, statistically significant variants in complex diseases to make predictions useful for personalized medicine. Why are significant variables not leading to good prediction of outcomes? We point out that this problem is prevalent in simple as well as complex data, in the sciences as well as the social sciences. We offer a brief explanation and some statistical insights on why higher significance cannot automatically imply stronger predictivity and illustrate through simulations and a real breast cancer example. We also demonstrate that highly predictive variables do not necessarily appear as highly significant, thus evading the researcher using significance-based methods. We point out that what makes variables good for prediction versus significance depends on different properties of the underlying distributions. If prediction is the goal, we must lay aside significance as the only selection standard. We suggest that progress in prediction requires efforts toward a new research agenda of searching for a novel criterion to retrieve highly predictive variables rather than highly significant variables. We offer an alternative approach that was not designed for significance, the partition retention method, which was very effective predicting on a long-studied breast cancer data set, by reducing the classification error rate from 30% to 8%.},
author = {Lo, Adeline and Chernoff, Herman and Zheng, Tian and Lo, Shaw-Hwa},
doi = {10.1073/pnas.1518285112},
isbn = {1518285112},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences},
number = {45},
pages = {13892--13897},
pmid = {26504198},
title = {{Why significant variables aren't automatically good predictors}},
url = {http://www.pnas.org/lookup/doi/10.1073/pnas.1518285112},
volume = {112},
year = {2015}
}
@article{Sullivan2012,
abstract = {Statistical significance is the least interesting thing about the results. You should describe the results in terms of measures of magnitude –not just, does a treatment affect people, but how much does it affect them. The primary product of a research inquiry is one or more measures of effect size, not P values.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Sullivan, Gail M and Feinn, Richard},
doi = {10.4300/JGME-D-12-00156.1},
eprint = {arXiv:1011.1669v3},
isbn = {1949-8349 1949-8357},
issn = {1949-8349},
journal = {Journal of Graduate Medical Education},
month = {sep},
number = {3},
pages = {279--282},
pmid = {23997866},
publisher = {Accreditation Council for Graduate Medical Education},
title = {{Using Effect Size—or Why the P Value Is Not Enough}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23997866 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC3444174 http://www.jgme.org/doi/abs/10.4300/JGME-D-12-00156.1},
volume = {4},
year = {2012}
}
@article{Szumilas2010,
abstract = {Understandig Odds Ratio},
author = {Szumilas, Magdalena},
doi = {10.1136/bmj.c4414},
isbn = {1719-8429},
issn = {17198429},
journal = {Journal of the Canadian Academy of Child and Adolescent Psychiatry},
month = {aug},
number = {3},
pages = {227--229},
pmid = {20842279},
publisher = {Canadian Academy of Child and Adolescent Psychiatry},
title = {{Explaining odds ratios}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/20842279 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC2938757},
volume = {19},
year = {2010}
}
@article{Diamond2013,
abstract = {Published reports of randomized clinical trials tend to emphasize the statistical significance of the treatment effect (p values) rather than its magnitude (effect size), although the clinical importance of the evidence depends more on the latter than on the former. We, therefore, compared the standard measures of effect size (relative and absolute risk reduction) and nonstandard composites of these measures (the product of the relative and absolute risk reductions and information content) with conventional assessments of statistical significance for 100 trials published in The New England Journal of Medicine. The p values were reported for 100% of the trials, relative risk reductions for 89%, and absolute risk reductions for 39%. Only 35% of trials reported both standard measures, and none reported either of the nonstandard measures. The standard measures correlated weakly (unexplained variance 77%). In contrast, the nonstandard measures correlated highly (unexplained variance 1.3%) but correlated weakly with statistical significance (unexplained variance 83%). Consequently, 25% of the trial results were adjudged "clinically unimportant" despite being "statistically significant." In conclusion, our results have shown that composite measures of effect size communicate the clinical importance of trial results better than do conventional assessments of risk reduction and statistical significance. {\textcopyright} 2013 Elsevier Inc. All rights reserved.},
author = {Diamond, George A. and Kaul, Sanjay},
doi = {10.1016/j.amjcard.2012.10.047},
file = {:Users/shahbook/Library/Application Support/Mendeley Desktop/Downloaded/Diamond, Kaul - 2013 - On reporting of effect size in randomized clinical trials.pdf:pdf},
issn = {00029149},
journal = {American Journal of Cardiology},
month = {feb},
number = {4},
pages = {613--617},
publisher = {Excerpta Medica},
title = {{On reporting of effect size in randomized clinical trials}},
url = {https://www-sciencedirect-com.proxy.library.emory.edu/science/article/pii/S000291491202334X?via%3Dihub},
volume = {111},
year = {2013}
}
@article{Blume2018,
abstract = {Verifying that a statistically significant result is scientifically meaningful is not only good scientific practice, it is a natural way to control the Type I error rate. Here we introduce a novel extension of the p-value—a second-generation p-value (p$\delta$)–that formally accounts for scientific relevance and leverages this natural Type I Error control. The approach relies on a pre-specified interval null hypothesis that represents the collection of effect sizes that are scientifically uninteresting or are practically null. The second-generation p-value is the proportion of data-supported hypotheses that are also null hypotheses. As such, second-generation p-values indicate when the data are compatible with null hypotheses (p$\delta$ = 1), or with alternative hypotheses (p$\delta$ = 0), or when the data are inconclusive (0 < p$\delta$ < 1). Moreover, second-generation p-values provide a proper scientific adjustment for multiple comparisons and reduce false discovery rates. This is an advance for environments rich in data, where traditional p-value adjustments are needlessly punitive. Second-generation p-values promote transparency, rigor and reproducibility of scientific results by a priori specifying which candidate hypotheses are practically meaningful and by providing a more reliable statistical summary of when the data are compatible with alternative or null hypotheses.},
archivePrefix = {arXiv},
arxivId = {1709.09333},
author = {Blume, Jeffrey D. and {D'Agostino McGowan}, Lucy and Dupont, William D. and Greevy, Robert A.},
doi = {10.1371/journal.pone.0188299},
editor = {Smalheiser, Neil R.},
eprint = {1709.09333},
isbn = {1111111111},
issn = {19326203},
journal = {PLoS ONE},
month = {mar},
number = {3},
pages = {e0188299},
pmid = {29565985},
publisher = {Public Library of Science},
title = {{Second-generation p-values: Improved rigor, reproducibility, & transparency in statistical analyses}},
url = {http://dx.plos.org/10.1371/journal.pone.0188299},
volume = {13},
year = {2018}
}
@article{Lee2016,
abstract = {The previous articles of the Statistical Round in the Korean Journal of Anesthesiology posed a strong enquiry on the issue of null hypothesis significance testing (NHST). P values lie at the core of NHST and are used to classify all treatments into two groups: "has a significant effect" or "does not have a significant effect." NHST is frequently criticized for its misinterpretation of relationships and limitations in assessing practical importance. It has now provoked criticism for its limited use in merely separating treatments that "have a significant effect" from others that do not. Effect sizes and CIs expand the approach to statistical thinking. These attractive estimates facilitate authors and readers to discriminate between a multitude of treatment effects. Through this article, I have illustrated the concept and estimating principles of effect sizes and CIs.},
author = {Lee, Dong Kyu},
doi = {10.4097/kjae.2016.69.6.555},
issn = {20057563},
journal = {Korean Journal of Anesthesiology},
keywords = {Confidence intervals,Effect sizes,P value},
month = {dec},
number = {6},
pages = {555--562},
pmid = {27924194},
publisher = {Korean Society of Anesthesiologists},
title = {{Alternatives to P value: Confidence interval and effect size}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/27924194 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC5133225},
volume = {69},
year = {2016}
}
