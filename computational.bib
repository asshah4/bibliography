@inproceedings{Abrishami2018,
  title = {P-{{QRS-T}} Localization in {{ECG}} Using Deep Learning},
  booktitle = {2018 {{IEEE EMBS International Conference}} on {{Biomedical}} \& {{Health Informatics}} ({{BHI}})},
  author = {Abrishami, Hedayat and Campbell, Matthew and Han, Chia and Czosek, Richard and Zhou, Xuefu},
  date = {2018-03},
  pages = {210--213},
  doi = {10.1109/BHI.2018.8333406},
  abstract = {This paper describes a work using the capabilities of deep neural networks to predict key wave locations in a cardiac complex on an electrocardiogram (ECG) as part of a challenge introduced by Physionet, a provider of ECG collections, on detecting critical waveforms that contain essential information in cardiology. The key waves include P-wave, QRS-wave, and T-wave. Recent attempts to extract hierarchical features of cardiac complexes have been reported in literature, but finding the accurate position of critical cardiac waves has been a challenge in the ECG signal processing research. This study investigates multiple architectures and learning rates of the deep neural networks and adopts a four-step procedure to find the best one that can predict the wave locations. A remarkable rate of 96.2\% of accuracy in the localization task has been achieved. This study consists of four parts to produce output predictions; obtaining the cardiac complexes from QT Databse (QTDB); introduce multiple architectures, including fully-connected networks, LeNet-style ConvNet with dropout, LeNet-style ConvNet without dropout and train these networks; use an unseen test set to calculate the accuracy of the system with different tolerance in each wave interval; compare all these architectures together to analyze the most suitable architecture for this task.},
  eventtitle = {2018 {{IEEE EMBS International Conference}} on {{Biomedical}} \& {{Health Informatics}} ({{BHI}})},
  keywords = {Biological neural networks,Computer architecture,Convolution,Electrocardiography,Feature extraction,Task analysis},
  file = {/Users/asshah4/projects/zotero/storage/5LSV65Z2/8333406.html}
}

@inproceedings{Ajdaraga2018b,
  title = {Analysis of Sampling Frequency and Resolution in {{ECG}} Signals},
  booktitle = {2017 25th {{Telecommunications Forum}}, {{TELFOR}} 2017 - {{Proceedings}}},
  author = {Ajdaraga, Era and Gusev, Marjan},
  date = {2018-11},
  volume = {2017-Janua},
  pages = {1--4},
  publisher = {{IEEE}},
  doi = {10.1109/TELFOR.2017.8249438},
  url = {https://ieeexplore-ieee-org.proxy.library.emory.edu/stamp/stamp.jsp?tp=&arnumber=8249438},
  urldate = {2020-01-28},
  abstract = {There are various ways a computer can detect QRS complexes, all of which depend on two main factors-the algorithm, and the data. In practice, doctors can pinpoint patient's R-peaks in an electrocardiogram by analyzing the complex morphology of the ECG signal; computers accomplish the same by utilizing different QRS detection algorithms. In this paper, we address the data problem to select the sample rate and resolution that obtain the highest accuracy. For this purpose, we conduct experiments to find the impact of the sampling frequency and resolution on the quality of the QRS detection, by several open-source QRS detection algorithms with multiple variations of data representation. The final goal is to recommend the lowest sampling frequency and smallest sampling resolution (bit depth), that will have sufficient data representation to enable high QRS detection accuracy.},
  isbn = {978-1-5386-3072-3},
  keywords = {ECG,electrocardiogram,sampling frequency,wearable telemedicine sensors}
}

@report{Akselrod1981,
  title = {Power {{Spectrum Analysis}} of {{Heart Rate Fluctuation}}: {{A Quantitative Probe}} of {{Beat-To-Beat Cardiovascular Control}}},
  author = {{Akselrod S.} and {Gordon D.} and {Ubel A.} and {Shannon D.} and {Barger C.} and {Cohen R.}},
  date = {1981},
  journaltitle = {Science},
  volume = {213},
  number = {4504},
  pages = {220--222},
  abstract = {Power spectrum analysis of heart rate fluctuations provides a quantitative means of assessing the functioning of the short-term cardiovascular control systems. We show that sympathetic and parasympathetic nervous activity make frequency-specific contributions to the heart rate power spectrum, and that renin-angiotensin system activity strongly modulates the amplitude of the spectral peak located at 0.04 Hz. Our data therefore provide evidence that the renin-angiotensin system plays a significant role in the short-term cardiovascular control on the time scale of seconds to minutes.},
  file = {/Users/asshah4/Library/CloudStorage/OneDrive-UniversityofIllinoisatChicago/articles/Akselrod S. et al_1981_Power Spectrum Analysis of Heart Rate Fluctuation.pdf}
}

@article{Ambrosy2021,
  title = {A {{Natural Language Processing-Based Approach}} for {{Identifying Hospitalizations}} for {{Worsening Heart Failure}} within an {{Integrated Health Care Delivery System}}},
  author = {Ambrosy, Andrew P. and Parikh, Rishi V. and Sung, Sue Hee and Narayanan, Anand and Masson, Rajeev and Lam, Phuong Quang and Kheder, Kevin and Iwahashi, Alan and Hardwick, Alexander B. and Fitzpatrick, Jesse K. and Avula, Harshith R. and Selby, Van N. and Shen, Xian and Sanghera, Navneet and Cristino, Joaquim and Go, Alan S.},
  date = {2021},
  journaltitle = {JAMA Network Open},
  eprint = {34807259},
  eprinttype = {pmid},
  publisher = {{American Medical Association}},
  issn = {25743805},
  doi = {10.1001/jamanetworkopen.2021.35152},
  abstract = {Importance: The current understanding of epidemiological mechanisms and temporal trends in hospitalizations for worsening heart failure (WHF) is based on claims and national reporting databases. However, these data sources are inherently limited by the accuracy and completeness of diagnostic coding and/or voluntary reporting. Objective: To assess the overall burden of and temporal trends in the rate of hospitalizations for WHF. Design, Setting, and Participants: This cohort study, performed from January 1, 2010, to December 31, 2019, used electronic health record (EHR) data from a large integrated health care delivery system. Exposures: Calendar year trends. Main Outcomes and Measures: Hospitalizations for WHF (ie, excluding observation stays) were defined as 1 symptom or more, 2 objective findings or more including 1 sign or more, and 2 doses or more of intravenous loop diuretics and/or new hemodialysis or continuous kidney replacement therapy. Symptoms and signs were identified using natural language processing (NLP) algorithms applied to EHR data. Results: The study population was composed of 118002 eligible patients experiencing 287992 unique hospitalizations (mean [SD] age, 75.6 [13.1] years; 147203 [51.1\%] male; 1655 [0.6\%] American Indian or Alaska Native, 28451 [9.9\%] Asian or Pacific Islander, 34903 [12.1\%] Black, 23452 [8.1\%] multiracial, 175840 [61.1\%] White, and 23691 [8.2\%] unknown), including 65357 with a principal discharge diagnosis and 222635 with a secondary discharge diagnosis of HF. The study population included 59868 patients (20.8\%) with HF with a reduced ejection fraction (HFrEF) ({$<$}40\%), 33361 (11.6\%) with HF with a midrange EF (HFmrEF) (40\%-49\%), 142347 (49.4\%) with HF with a preserved EF (HFpEF) (≥50\%), and 52416 (18.2\%) with unknown EF. A total of 58042 admissions (88.8\%) with a primary discharge diagnosis of HF and 62764 admissions (28.2\%) with a secondary discharge diagnosis of HF met the prespecified diagnostic criteria for WHF. Overall, hospitalizations for WHF identified on NLP-based algorithms increased from 5.2 to 7.6 per 100 hospitalizations per year during the study period. Subgroup analyses found an increase in hospitalizations for WHF based on NLP from 1.5 to 1.9 per 100 hospitalizations for HFrEF, from 0.6 to 1.0 per 100 hospitalizations for HFmrEF, and from 2.6 to 3.9 per 100 hospitalizations for HFpEF. Conclusions and Relevance: The findings of this cohort study suggest that the burden of hospitalizations for WHF may be more than double that previously estimated using only principal discharge diagnosis. There has been a gradual increase in the rate of hospitalizations for WHF with a more noticeable increase observed for HFpEF..},
  file = {/Users/asshah4/Library/CloudStorage/OneDrive-UniversityofIllinoisatChicago/articles/Ambrosy et al_2021_A Natural Language Processing-Based Approach for Identifying Hospitalizations.pdf}
}

@article{Amorim2015a,
  title = {Modelling Recurrent Events: {{A}} Tutorial for Analysis in Epidemiology},
  author = {Amorim, Leila D.A.F. and Cai, Jianwen},
  date = {2015},
  journaltitle = {International Journal of Epidemiology},
  volume = {44},
  number = {1},
  pages = {324--333},
  issn = {14643685},
  doi = {10.1093/ije/dyu222},
  url = {https://academic.oup.com/ije/article-abstract/44/1/324/654595},
  urldate = {2019-12-21},
  abstract = {In many biomedical studies, the event of interest can occur more than once in a participant. These events are termed recurrent events. However, the majority of analyses focus only on time to the first event, ignoring the subsequent events. Several statistical models have been proposed for analysing multiple events. In this paper we explore and illustrate several modelling techniques for analysis of recurrent time-to-event data, including conditional models for multivariate survival data (AG, PWP-TT and PWP-GT), marginal means/ rates models, frailty and multi-state models. We also provide a tutorial for analysing such type of data, with three widely used statistical software programmes. Different approaches and software are illustrated using data from a bladder cancer project and from a study on lower respiratory tract infection in children in Brazil. Finally, we make recommendations for modelling strategy selection for analysis of recurrent event data.},
  keywords = {Recurrent events,Survival modelling,Time-to-event data},
  file = {/Users/asshah4/Library/CloudStorage/OneDrive-UniversityofIllinoisatChicago/articles/Amorim_Cai_2015_Modelling recurrent events.pdf}
}

@article{Amrhein2019,
  title = {Scientists Rise up against Statistical Significance},
  author = {Amrhein, Valentin and Greenland, Sander and McShane, Blake},
  date = {2019-03},
  journaltitle = {Nature},
  volume = {567},
  number = {7748},
  pages = {305--307},
  publisher = {{Nature Publishing Group}},
  doi = {10.1038/d41586-019-00857-9},
  url = {https://www.nature.com/articles/d41586-019-00857-9},
  urldate = {2023-10-25},
  abstract = {Valentin Amrhein, Sander Greenland, Blake McShane and more than 800 signatories call for an end to hyped claims and the dismissal of possibly crucial effects.},
  issue = {7748},
  langid = {english},
  keywords = {Research data,Research management},
  annotation = {Bandiera\_abtest: a Cg\_type: Comment Subject\_term: Research data, Research management},
  file = {/Users/asshah4/Library/CloudStorage/OneDrive-UniversityofIllinoisatChicago/articles/Amrhein et al_2019_Scientists rise up against statistical significance.pdf;/Users/asshah4/projects/zotero/storage/7YN9IX9Z/d41586-019-00857-9.html}
}

@article{Attia2019,
  title = {Age and {{Sex Estimation Using Artificial Intelligence From Standard}} 12-{{Lead ECGs}}},
  author = {Attia, Zachi I. and Friedman, Paul A. and Noseworthy, Peter A. and Lopez-Jimenez, Francisco and Ladewig, Dorothy J. and Satam, Gaurav and Pellikka, Patricia A. and Munger, Thomas M. and Asirvatham, Samuel J. and Scott, Christopher G. and Carter, Rickey E. and Kapa, Suraj},
  date = {2019},
  journaltitle = {Circulation. Arrhythmia and electrophysiology},
  volume = {12},
  number = {9},
  pages = {e007284},
  issn = {19413084},
  doi = {10.1161/CIRCEP.119.007284},
  abstract = {BACKGROUND: Sex and age have long been known to affect the ECG. Several biologic variables and anatomic factors may contribute to sex and age-related differences on the ECG. We hypothesized that a convolutional neural network (CNN) could be trained through a process called deep learning to predict a person's age and self-reported sex using only 12-lead ECG signals. We further hypothesized that discrepancies between CNN-predicted age and chronological age may serve as a physiological measure of health. METHODS: We trained CNNs using 10-second samples of 12-lead ECG signals from 499\,727 patients to predict sex and age. The networks were tested on a separate cohort of 275\,056 patients. Subsequently, 100 randomly selected patients with multiple ECGs over the course of decades were identified to assess within-individual accuracy of CNN age estimation. RESULTS: Of 275\,056 patients tested, 52\% were males and mean age was 58.6±16.2 years. For sex classification, the model obtained 90.4\% classification accuracy with an area under the curve of 0.97 in the independent test data. Age was estimated as a continuous variable with an average error of 6.9±5.6 years (R-squared =0.7). Among 100 patients with multiple ECGs over the course of at least 2 decades of life, most patients (51\%) had an average error between real age and CNN-predicted age of {$<$}7 years. Major factors seen among patients with a CNN-predicted age that exceeded chronologic age by {$>$}7 years included: low ejection fraction, hypertension, and coronary disease (P{$<$}0.01). In the 27\% of patients where correlation was {$>$}0.8 between CNN-predicted and chronologic age, no incident events occurred over follow-up (33±12 years). CONCLUSIONS: Applying artificial intelligence to the ECG allows prediction of patient sex and estimation of age. The ability of an artificial intelligence algorithm to determine physiological age, with further validation, may serve as a measure of overall health.},
  keywords = {artificial intelligence,coronary disease,electrocardiography,hypertension,neural network},
  file = {/Users/asshah4/Library/CloudStorage/OneDrive-UniversityofIllinoisatChicago/articles/Attia et al_2019_Age and Sex Estimation Using Artificial Intelligence From Standard 12-Lead ECGs.pdf}
}

@article{Attia2019a,
  title = {An Artificial Intelligence-Enabled {{ECG}} Algorithm for the Identification of Patients with Atrial Fibrillation during Sinus Rhythm: A Retrospective Analysis of Outcome Prediction},
  shorttitle = {An Artificial Intelligence-Enabled {{ECG}} Algorithm for the Identification of Patients with Atrial Fibrillation during Sinus Rhythm},
  author = {Attia, Zachi I. and Noseworthy, Peter A. and Lopez-Jimenez, Francisco and Asirvatham, Samuel J. and Deshmukh, Abhishek J. and Gersh, Bernard J. and Carter, Rickey E. and Yao, Xiaoxi and Rabinstein, Alejandro A. and Erickson, Brad J. and Kapa, Suraj and Friedman, Paul A.},
  date = {2019-09-07},
  journaltitle = {The Lancet},
  shortjournal = {The Lancet},
  volume = {394},
  number = {10201},
  eprint = {31378392},
  eprinttype = {pmid},
  pages = {861--867},
  publisher = {{Elsevier}},
  issn = {0140-6736, 1474-547X},
  doi = {10.1016/S0140-6736(19)31721-0},
  url = {https://www.thelancet.com/journals/lancet/article/PIIS0140-6736(19)31721-0/fulltext},
  urldate = {2023-09-06},
  langid = {english},
  file = {/Users/asshah4/projects/zotero/storage/94Y7D2NA/Attia et al. - 2019 - An artificial intelligence-enabled ECG algorithm f.pdf}
}

@article{Austin2016,
  title = {Introduction to the {{Analysis}} of {{Survival Data}} in the {{Presence}} of {{Competing Risks}}},
  author = {Austin, Peter C. and Lee, Douglas S. and Fine, Jason P.},
  date = {2016},
  journaltitle = {Circulation},
  volume = {133},
  number = {6},
  eprint = {26858290},
  eprinttype = {pmid},
  pages = {601--609},
  issn = {15244539},
  doi = {10.1161/CIRCULATIONAHA.115.017719},
  abstract = {Competing risks occur frequently in the analysis of survival data. A competing risk is an event whose occurrence precludes the occurrence of the primary event of interest. In a study examining time to death attributable to cardiovascular causes, death attributable to noncardiovascular causes is a competing risk. When estimating the crude incidence of outcomes, analysts should use the cumulative incidence function, rather than the complement of the Kaplan-Meier survival function. The use of the Kaplan-Meier survival function results in estimates of incidence that are biased upward, regardless of whether the competing events are independent of one another. When fitting regression models in the presence of competing risks, researchers can choose from 2 different families of models: modeling the effect of covariates on the cause-specific hazard of the outcome or modeling the effect of covariates on the cumulative incidence function. The former allows one to estimate the effect of the covariates on the rate of occurrence of the outcome in those subjects who are currently event free. The latter allows one to estimate the effect of covariates on the absolute risk of the outcome over time. The former family of models may be better suited for addressing etiologic questions, whereas the latter model may be better suited for estimating a patient's clinical prognosis. We illustrate the application of these methods by examining cause-specific mortality in patients hospitalized with heart failure. Statistical software code in both R and SAS is provided.},
  keywords = {cumulative incidence function,{data interpretation, statistical},incidence,{models, statistical},proportional hazards models,risk assessment,survival analysis},
  file = {/Users/asshah4/Library/CloudStorage/OneDrive-UniversityofIllinoisatChicago/articles/Austin et al_2016_Introduction to the Analysis of Survival Data in the Presence of Competing Risks.pdf}
}

@article{Austin2016a,
  title = {Introduction to the {{Analysis}} of {{Survival Data}} in the {{Presence}} of {{Competing Risks}}},
  author = {Austin, Peter C. and Lee, Douglas S. and Fine, Jason P.},
  date = {2016-02-09},
  journaltitle = {Circulation},
  volume = {133},
  number = {6},
  pages = {601--609},
  publisher = {{American Heart Association}},
  doi = {10.1161/CIRCULATIONAHA.115.017719},
  url = {https://www.ahajournals.org/doi/10.1161/CIRCULATIONAHA.115.017719},
  urldate = {2023-09-13},
  abstract = {Competing risks occur frequently in the analysis of survival data. A competing risk is an event whose occurrence precludes the occurrence of the primary event of interest. In a study examining time to death attributable to cardiovascular causes, death attributable to noncardiovascular causes is a competing risk. When estimating the crude incidence of outcomes, analysts should use the cumulative incidence function, rather than the complement of the Kaplan-Meier survival function. The use of the Kaplan-Meier survival function results in estimates of incidence that are biased upward, regardless of whether the competing events are independent of one another. When fitting regression models in the presence of competing risks, researchers can choose from 2 different families of models: modeling the effect of covariates on the cause-specific hazard of the outcome or modeling the effect of covariates on the cumulative incidence function. The former allows one to estimate the effect of the covariates on the rate of occurrence of the outcome in those subjects who are currently event free. The latter allows one to estimate the effect of covariates on the absolute risk of the outcome over time. The former family of models may be better suited for addressing etiologic questions, whereas the latter model may be better suited for estimating a patient’s clinical prognosis. We illustrate the application of these methods by examining cause-specific mortality in patients hospitalized with heart failure. Statistical software code in both R and SAS is provided.},
  keywords = {cumulative incidence function,{data interpretation, statistical},incidence,{models, statistical},proportional hazards models,risk assessment,survival,survival analysis},
  file = {/Users/asshah4/Library/CloudStorage/OneDrive-UniversityofIllinoisatChicago/articles/Austin et al_2016_Introduction to the Analysis of Survival Data in the Presence of Competing Risks2.pdf}
}

@article{Carlin2005,
  title = {Regression Models for Twin Studies: {{A}} Critical Review},
  author = {Carlin, John B and Gurrin, Lyle C and Sterne, Jonathan AC C and Morley, Ruth and Dwyer, Terry},
  date = {2005-10-01},
  journaltitle = {International Journal of Epidemiology},
  volume = {34},
  number = {5},
  eprint = {16087687},
  eprinttype = {pmid},
  pages = {1089--1099},
  publisher = {{Oxford University Press on}},
  issn = {03005771},
  doi = {10.1093/ije/dyi153},
  url = {http://academic.oup.com/ije/article/34/5/1089/645923/Regression-models-for-twin-studies-a-critical},
  urldate = {2021-03-11},
  abstract = {Twin studies have long been recognized for their value in learning about the aetiology of disease and specifically for their potential for separating genetic effects from environmental effects. The recent upsurge of interest in life-course epidemiology and the study of developmental influences on later health has provided a new impetus to study twins as a source of unique insights. Twins are of special interest because they provide naturally matched pairs where the confounding effects of a large number of potentially causal factors (such as maternal nutrition or gestation length) may be removed by comparisons between twins who share them. The traditional tool of epidemiological ‘risk factor analysis’ is the regression model, but it is not straightforward to transfer standard regression methods to twin data, because the analysis needs to reflect the paired structure of the data, which induces correlation between twins. This paper reviews the use of more specialized regression methods for twin data, based on generalized least squares or linear mixed models, and explains the relationship between these methods and the commonly used approach of analysing within-twin-pair difference values. Methods and issues of interpretation are illustrated using an example from a recent study of the association between birth weight and cord blood erythropoietin. We focus on the analysis of continuous outcome measures but review additional complexities that arise with binary outcomes. We recommend the use of a general model that includes separate regression coefficients for within-twin-pair and between-pair effects, and provide guidelines for the interpretation of estimates obtained under this model.},
  isbn = {0300-5771 (Print)\textbackslash r0300-5771},
  keywords = {Data correlation,Regression models,Statistics,Twin studies},
  file = {/Users/asshah4/Library/CloudStorage/OneDrive-UniversityofIllinoisatChicago/articles/Carlin et al_2005_Regression models for twin studies.pdf}
}

@article{Choi2020a,
  title = {A Guide to Performing {{Polygenic Risk Score}} Analyses},
  author = {Choi, Shing Wan and Mak, Timothy Shin Heng and O’Reilly, Paul F.},
  date = {2020-09-01},
  journaltitle = {Nature protocols},
  shortjournal = {Nat Protoc},
  volume = {15},
  number = {9},
  eprint = {32709988},
  eprinttype = {pmid},
  pages = {2759--2772},
  issn = {1754-2189},
  doi = {10.1038/s41596-020-0353-1},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7612115/},
  urldate = {2023-07-27},
  abstract = {The application of polygenic risk scores (PRS) has become routine across biomedical research. Among a range of applications, PRS are exploited to assess shared aetiology between phenotypes, to evaluate the clinical utility of genetic data for common disease, and as part of experimental studies in which, for example, experiments are performed on individuals, or their biological samples (e.g. tissues, cells), at the tails of the PRS distribution and contrasted. As GWAS sample sizes increase and PRS become more powerful, they are set to play a key role in research and personalised medicine. However, despite the growing application and importance of PRS, there are limited guidelines for performing PRS analyses, which can lead to inconsistency between studies and misinterpretation of results. Here we provide detailed guidelines for performing polygenic risk score analyses. We discuss different methods for the calculation of PRS, outline standard quality control steps, provide an introductory online tutorial, highlight common misconceptions relating to PRS results, offer recommendations for best-practice and discuss future challenges.},
  pmcid = {PMC7612115},
  file = {/Users/asshah4/Library/CloudStorage/OneDrive-UniversityofIllinoisatChicago/articles/Choi et al_2020_A guide to performing Polygenic Risk Score analyses.pdf}
}

@article{Collister2022,
  title = {Calculating {{Polygenic Risk Scores}} ({{PRS}}) in {{UK Biobank}}: {{A Practical Guide}} for {{Epidemiologists}}},
  shorttitle = {Calculating {{Polygenic Risk Scores}} ({{PRS}}) in {{UK Biobank}}},
  author = {Collister, Jennifer A. and Liu, Xiaonan and Clifton, Lei},
  date = {2022},
  journaltitle = {Frontiers in Genetics},
  volume = {13},
  issn = {1664-8021},
  url = {https://www.frontiersin.org/articles/10.3389/fgene.2022.818574},
  urldate = {2023-10-12},
  abstract = {A polygenic risk score estimates the genetic risk of an individual for some disease or trait, calculated by aggregating the effect of many common variants associated with the condition. With the increasing availability of genetic data in large cohort studies such as the UK Biobank, inclusion of this genetic risk as a covariate in statistical analyses is becoming more widespread. Previously this required specialist knowledge, but as tooling and data availability have improved it has become more feasible for statisticians and epidemiologists to calculate existing scores themselves for use in analyses. While tutorial resources exist for conducting genome-wide association studies and generating of new polygenic risk scores, fewer guides exist for the simple calculation and application of existing genetic scores. This guide outlines the key steps of this process: selection of suitable polygenic risk scores from the literature, extraction of relevant genetic variants and verification of their quality, calculation of the risk score and key considerations of its inclusion in statistical models, using the UK Biobank imputed data as a model data set. Many of the techniques in this guide will generalize to other datasets, however we also focus on some of the specific techniques required for using data in the formats UK Biobank have selected. This includes some of the challenges faced when working with large numbers of variants, where the computation time required by some tools is impractical. While we have focused on only a couple of tools, which may not be the best ones for every given aspect of the process, one barrier to working with genetic data is the sheer volume of tools available, and the difficulty for a novice to assess their viability. By discussing in depth a couple of tools that are adequate for the calculation even at large scale, we hope to make polygenic risk scores more accessible to a wider range of researchers.},
  file = {/Users/asshah4/Library/CloudStorage/OneDrive-UniversityofIllinoisatChicago/articles/Collister et al_2022_Calculating Polygenic Risk Scores (PRS) in UK Biobank.pdf}
}

@article{Cox1972,
  title = {Regression {{Models}} and {{Life-Tables}}},
  author = {Cox, D. R.},
  date = {1972-01},
  journaltitle = {Journal of the Royal Statistical Society: Series B (Methodological)},
  volume = {34},
  number = {2},
  pages = {187--202},
  publisher = {{Wiley}},
  doi = {10.1111/j.2517-6161.1972.tb00899.x},
  abstract = {The abalysis of censored failure times is considered . assumed on each individual are available values of on or more explanatory variables. the hazard function is taken to be function of the explanatory variables and unknown function of time. a conditional likelihood is obtained lading to ibferense about the unknown regression coffcients.},
  file = {/Users/asshah4/Library/CloudStorage/OneDrive-UniversityofIllinoisatChicago/articles/Cox_1972_Regression Models and Life-Tables.pdf}
}

@article{Darmawahyuni2023,
  title = {Improved Delineation Model of a Standard 12-Lead Electrocardiogram Based on a Deep Learning Algorithm},
  author = {Darmawahyuni, Annisa and Nurmaini, Siti and Rachmatullah, Muhammad Naufal and Avi, Prazna Paramitha and Teguh, Samuel Benedict Putra and Sapitri, Ade Iriani and Tutuko, Bambang and Firdaus, Firdaus},
  date = {2023-07-28},
  journaltitle = {BMC Medical Informatics and Decision Making},
  shortjournal = {BMC Medical Informatics and Decision Making},
  volume = {23},
  number = {1},
  pages = {139},
  issn = {1472-6947},
  doi = {10.1186/s12911-023-02233-0},
  url = {https://doi.org/10.1186/s12911-023-02233-0},
  urldate = {2023-08-14},
  abstract = {Signal delineation of a standard 12-lead electrocardiogram (ECG) is a decisive step for retrieving complete information and extracting signal characteristics for each lead in cardiology clinical practice. However, it is arduous to manually assess the leads, as a variety of signal morphological variations in each lead have potential defects in recording, noise, or irregular heart rhythm/beat.},
  keywords = {12-lead electrocardiogram,Bidirectional long short-term memory,Convolutional neural network,Delineation model,ECG waveform,Isoelectric line},
  file = {/Users/asshah4/Library/CloudStorage/OneDrive-UniversityofIllinoisatChicago/articles/Darmawahyuni et al_2023_Improved delineation model of a standard 12-lead electrocardiogram based on a.pdf;/Users/asshah4/projects/zotero/storage/GEDX7SWT/s12911-023-02233-0.html}
}

@article{Dudbridge2013,
  title = {Power and {{Predictive Accuracy}} of {{Polygenic Risk Scores}}},
  author = {Dudbridge, Frank},
  date = {2013-03-21},
  journaltitle = {PLoS Genetics},
  shortjournal = {PLoS Genet},
  volume = {9},
  number = {3},
  eprint = {23555274},
  eprinttype = {pmid},
  pages = {e1003348},
  issn = {1553-7390},
  doi = {10.1371/journal.pgen.1003348},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3605113/},
  urldate = {2023-10-12},
  abstract = {Polygenic scores have recently been used to summarise genetic effects among an ensemble of markers that do not individually achieve significance in a large-scale association study. Markers are selected using an initial training sample and used to construct a score in an independent replication sample by forming the weighted sum of associated alleles within each subject. Association between a trait and this composite score implies that a genetic signal is present among the selected markers, and the score can then be used for prediction of individual trait values. This approach has been used to obtain evidence of a genetic effect when no single markers are significant, to establish a common genetic basis for related disorders, and to construct risk prediction models. In some cases, however, the desired association or prediction has not been achieved. Here, the power and predictive accuracy of a polygenic score are derived from a quantitative genetics model as a function of the sizes of the two samples, explained genetic variance, selection thresholds for including a marker in the score, and methods for weighting effect sizes in the score. Expressions are derived for quantitative and discrete traits, the latter allowing for case/control sampling. A novel approach to estimating the variance explained by a marker panel is also proposed. It is shown that published studies with significant association of polygenic scores have been well powered, whereas those with negative results can be explained by low sample size. It is also shown that useful levels of prediction may only be approached when predictors are estimated from very large samples, up to an order of magnitude greater than currently available. Therefore, polygenic scores currently have more utility for association testing than predicting complex traits, but prediction will become more feasible as sample sizes continue to grow., Recently there has been much interest in combining multiple genetic markers into a single score for predicting disease risk. Even if many of the individual markers have no detected effect, the combined score could be a strong predictor of disease. This has allowed researchers to demonstrate that some diseases have a strong genetic basis, even if few actual genes have been identified, and it has also revealed a common genetic basis for distinct diseases. These analyses have so far been performed opportunistically, with mixed results. Here I derive formulae based on the heritability of disease and size of the study, allowing researchers to plan their analyses from a more informed position. I show that discouraging results in some previous studies were due to the low number of subjects studied, but a modest increase in study size would allow more successful analysis. However, I also show that, for genetics to become useful for predicting individual risk of disease, hundreds of thousands of subjects may be needed to estimate the gene effects. This is larger than most existing studies, but will become more common in the near future, so that gene scores will become more useful for predicting disease than has appeared to date.},
  pmcid = {PMC3605113},
  file = {/Users/asshah4/Library/CloudStorage/OneDrive-UniversityofIllinoisatChicago/articles/Dudbridge_2013_Power and Predictive Accuracy of Polygenic Risk Scores.pdf}
}

@article{Duraj2022,
  title = {Semantic {{Segmentation}} of 12-{{Lead ECG Using 1D Residual U-Net}} with {{Squeeze-Excitation Blocks}}},
  author = {Duraj, Konrad and Piaseczna, Natalia and Kostka, Paweł and Tkacz, Ewaryst},
  date = {2022-01},
  journaltitle = {Applied Sciences},
  volume = {12},
  number = {7},
  pages = {3332},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {2076-3417},
  doi = {10.3390/app12073332},
  url = {https://www.mdpi.com/2076-3417/12/7/3332},
  urldate = {2023-08-14},
  abstract = {Analyzing biomedical data is a complex task that requires specialized knowledge. The development of knowledge and technology in the field of deep machine learning creates an opportunity to try and transfer human knowledge to the computer. In turn, this fact influences the development of systems for the automatic evaluation of the patient’s health based on data acquired from sensors. Electrocardiography (ECG) is a technique that enables visualizing the electrical activity of the heart in a noninvasive way, using electrodes placed on the surface of the skin. This signal carries a lot of information about the condition of heart muscle. The aim of this work is to create a system for semantic segmentation of the ECG signal. For this purpose, we used a database from Lobachevsky University available on Physionet, containing 200, 10-second, and 12-lead ECG signals with annotations, and applied one-dimensional U-Net with the addition of squeeze-excitation blocks. The created model achieved a set of parameters indicating high performance (for the test set: accuracy—0.95, AUC—0.99, specificity—0.95, sensitivity—0.99) in extracting characteristic parts of ECG signal such as P and T-waves and QRS complex, regardless of the lead.},
  issue = {7},
  langid = {english},
  keywords = {deep learning,ECG,signal segmentation},
  file = {/Users/asshah4/Library/CloudStorage/OneDrive-UniversityofIllinoisatChicago/articles/Duraj et al_2022_Semantic Segmentation of 12-Lead ECG Using 1D Residual U-Net with.pdf}
}

@article{Garcia-Lopez2020,
  title = {Extracting the {{Jugular Venous Pulse}} from {{Anterior Neck Contact Photoplethysmography}}},
  author = {García-López, Irene and Rodriguez-Villegas, Esther},
  date = {2020-02-26},
  journaltitle = {Scientific Reports},
  shortjournal = {Sci Rep},
  volume = {10},
  number = {1},
  pages = {3466},
  publisher = {{Nature Publishing Group}},
  issn = {2045-2322},
  doi = {10.1038/s41598-020-60317-7},
  url = {https://www.nature.com/articles/s41598-020-60317-7},
  urldate = {2023-10-10},
  abstract = {The jugular venous pulse (JVP) is the reference physiological signal used to detect right atrial and central venous pressure (CVP) abnormalities in cardio-vascular diseases (CVDs) diagnosis. Invasive central venous line catheterization has always been the gold standard method to extract it reliably. However, due to all the risks it entails, novel non-invasive approaches, exploiting distance cameras and lasers, have recently arisen to measure the JVP at the external and internal jugular veins. These remote options however, constraint patients to very specific body positions in front of the imaging system, making it inadequate for long term monitoring. In this study, we demonstrate, for the first time, that reflectance photoplethysmography (PPG) can be an alternative for extracting the JVP from the anterior jugular veins, in a contact manner. Neck JVP-PPG signals were recorded from 20 healthy participants, together with reference ECG and arterial finger PPG signals for validation. B-mode ultrasound imaging of the internal jugular vein also proved the validity of the proposed method. The results show that is possible to identify the characteristic a, c, v pressure waves in the novel signals, and confirm their cardiac-cycle timings in consistency with established cardiac physiology. Wavelet coherence values (close to 1 and phase shifts of ±180°) corroborated that neck contact JVP-PPG pulses were negatively correlated with arterial finger PPG. Average JVP waveforms for each subject showed typical JVP pulses contours except for the singularity of an unknown "u" wave occurring after the c wave, in half of the cohort. This work is of great significance for the future of CVDs diagnosis, as it has the potential to reduce the risks associated with conventional catheterization and enable continuous non-invasive point-of-care monitoring of CVP, without restricting patients to limited postures.},
  issue = {1},
  langid = {english},
  keywords = {Cardiology,Cardiovascular diseases,Diagnosis,Medical imaging,Prognostic markers},
  file = {/Users/asshah4/Library/CloudStorage/OneDrive-UniversityofIllinoisatChicago/articles/García-López_Rodriguez-Villegas_2020_Extracting the Jugular Venous Pulse from Anterior Neck Contact.pdf}
}

@article{Gignac2016a,
  title = {Effect Size Guidelines for Individual Differences Researchers},
  author = {Gignac, Gilles E. and Szodorai, Eva T.},
  date = {2016-11-01},
  journaltitle = {Personality and Individual Differences},
  volume = {102},
  pages = {74--78},
  publisher = {{Elsevier Ltd}},
  issn = {01918869},
  doi = {10.1016/j.paid.2016.06.069},
  abstract = {Individual differences researchers very commonly report Pearson correlations between their variables of interest. Cohen (1988) provided guidelines for the purposes of interpreting the magnitude of a correlation, as well as estimating power. Specifically, r = 0.10, r = 0.30, and r = 0.50 were recommended to be considered small, medium, and large in magnitude, respectively. However, Cohen's effect size guidelines were based principally upon an essentially qualitative impression, rather than a systematic, quantitative analysis of data. Consequently, the purpose of this investigation was to develop a large sample of previously published meta-analytically derived correlations which would allow for an evaluation of Cohen's guidelines from an empirical perspective. Based on 708 meta-analytically derived correlations, the 25th, 50th, and 75th percentiles corresponded to correlations of 0.11, 0.19, and 0.29, respectively. Based on the results, it is suggested that Cohen's correlation guidelines are too exigent, as {$<$} 3\% of correlations in the literature were found to be as large as r = 0.50. Consequently, in the absence of any other information, individual differences researchers are recommended to consider correlations of 0.10, 0.20, and 0.30 as relatively small, typical, and relatively large, in the context of a power analysis, as well as the interpretation of statistical results from a normative perspective.},
  keywords = {Correlations,Effect size,Guidelines}
}

@article{Goldberger2000,
  title = {{{PhysioBank}}, {{PhysioToolkit}}, and {{PhysioNet}}: Components of a New Research Resource for Complex Physiologic Signals.},
  author = {Goldberger, Ary L and Amaral, L. A. and Glass, L. and Hausdorff, Jeffrey M and Ivanov, P. C. and Mark, Roger G and Mietus, Joseph E and Moody, George B and Peng, C. K. and Stanley, H. E.},
  date = {2000},
  journaltitle = {Circulation},
  volume = {101},
  number = {23},
  eprint = {10851218},
  eprinttype = {pmid},
  issn = {15244539},
  doi = {10.1161/01.cir.101.23.e215},
  url = {http://www.physionet.org},
  urldate = {2020-03-30},
  abstract = {The newly inaugurated Research Resource for Complex Physiologic Signals, which was created under the auspices of the National Center for Research Resources of the National Institutes of Health, is intended to stimulate current research and new investigations in the study of cardiovascular and other complex biomedical signals. The resource has 3 interdependent components. PhysioBank is a large and growing archive of well-characterized digital recordings of physiological signals and related data for use by the biomedical research community. It currently includes databases of multiparameter cardiopulmonary, neural, and other biomedical signals from healthy subjects and from patients with a variety of conditions with major public health implications, including life-threatening arrhythmias, congestive heart failure, sleep apnea, neurological disorders, and aging. PhysioToolkit is a library of open-source software for physiological signal processing and analysis, the detection of physiologically significant events using both classic techniques and novel methods based on statistical physics and nonlinear dynamics, the interactive display and characterization of signals, the creation of new databases, the simulation of physiological and other signals, the quantitative evaluation and comparison of analysis methods, and the analysis of nonstationary processes. PhysioNet is an on-line forum for the dissemination and exchange of recorded biomedical signals and open-source software for analyzing them. It provides facilities for the cooperative analysis of data and the evaluation of proposed new algorithms. In addition to providing free electronic access to PhysioBank data and PhysioToolkit software via the World Wide Web (http://www.physionet. org), PhysioNet offers services and training via on-line tutorials to assist users with varying levels of expertise.},
  keywords = {aging databases death,autonomic nonlinear dynamics,sudden electrophysiology heart rate nervous system},
  file = {/Users/asshah4/Library/CloudStorage/OneDrive-UniversityofIllinoisatChicago/articles/Goldberger et al_2000_PhysioBank, PhysioToolkit, and PhysioNet.pdf}
}

@article{Haber2022,
  title = {{{DAG With Omitted Objects Displayed}} ({{DAGWOOD}}): A Framework for Revealing Causal Assumptions in {{DAGs}}},
  author = {Haber, Noah A. and Wood, Mollie E. and Wieten, Sarah and Breskin, Alexander},
  date = {2022-04-01},
  journaltitle = {Annals of Epidemiology},
  volume = {68},
  eprint = {35124197},
  eprinttype = {pmid},
  pages = {64--71},
  publisher = {{Elsevier Inc.}},
  issn = {18732585},
  doi = {10.1016/j.annepidem.2022.01.001},
  abstract = {Directed acyclic graphs (DAGs) are frequently used in epidemiology as a method to encode causal inference assumptions. We propose the DAGWOOD framework to bring many of those encoded assumptions to the forefront. DAGWOOD combines a root DAG (the DAG in the proposed analysis) and a set of branch DAGs (alternative hidden assumptions to the root DAG). All branch DAGs share a common ruleset, and must 1) change the root DAG, 2) be a valid DAG, and either 3a) change the minimally sufficient adjustment set or 3b) change the number of frontdoor paths. Branch DAGs comprise a list of assumptions which must be justified as negligible. We define two types of branch DAGs: exclusion branch DAGs add a single- or bidirectional pathway between two nodes in the root DAG (e.g., direct pathways and colliders), while misdirection branch DAGs represent alternative pathways that could be drawn between objects (e.g., creating a collider by reversing the direction of causation for a controlled confounder). The DAGWOOD framework 1) organizes causal model assumptions, 2) reinforces best DAG practices, 3) provides a framework for evaluation of causal models, and 4) can be used for generating causal models.},
  keywords = {causal inference,DAG,methods},
  file = {/Users/asshah4/Library/CloudStorage/OneDrive-UniversityofIllinoisatChicago/articles/Haber et al_2022_DAG With Omitted Objects Displayed (DAGWOOD).pdf}
}

@inproceedings{Haleem2022,
  title = {A {{Deep Learning Based ECG Segmentation Tool}} for {{Detection}} of {{ECG Beat Parameters}}},
  booktitle = {2022 {{IEEE Symposium}} on {{Computers}} and {{Communications}} ({{ISCC}})},
  author = {Haleem, Muhammad Salman and Pecchia, Leandro},
  date = {2022-06},
  pages = {1--4},
  issn = {2642-7389},
  doi = {10.1109/ISCC55528.2022.9912906},
  abstract = {The role of ECG segmentation tool has been pivotal in automated analysis of real-time ECG signals for detection of non-invasive cardiovascular and physiological conditions. Most of the existing approaches focus on traditional signal processing and/or traditional machine learning based approaches which are highly dependent on signal noise, inter/intra subject variability, etc. With the advent of deep learning based networks, it is possible to design and develop the classification model based on local features along with spatial and temporal context of the physiological signals. In this paper, we developed the attention based Convolutional Bidirectional Long Short Term Memory (Conv-BiLSTM) architecture network based on local beat features and temporal sequencing while correlating ECG beat across different positions. The performance of our ECG segmentation tool has been evaluated against the state-of-the art approaches in terms of ECG segmentation and fiducial point detection accuracy. The ECG segmentation accuracy was 95\% whereas fiducial point detection accuracy was 99.4\%.},
  eventtitle = {2022 {{IEEE Symposium}} on {{Computers}} and {{Communications}} ({{ISCC}})},
  keywords = {Convolutional neural networks,Deep learning,ECG parameter detection,ECG segmentation,Electrocardiography,Feature extraction,fiducial point detection,Memory architecture,Real-time systems,Sequential analysis},
  file = {/Users/asshah4/Library/CloudStorage/OneDrive-UniversityofIllinoisatChicago/articles/Haleem_Pecchia_2022_A Deep Learning Based ECG Segmentation Tool for Detection of ECG Beat Parameters.pdf;/Users/asshah4/projects/zotero/storage/U5C4KBTU/stamp.html}
}

@article{Hamner2001,
  title = {Automated Quantification of Sympathetic Beat-by-Beat Activity, Independent of Signal Quality},
  author = {Hamner, J. W. and Taylor, J. Andrew},
  date = {2001},
  journaltitle = {Journal of Applied Physiology},
  volume = {91},
  number = {3},
  pages = {1199--1206},
  issn = {87507587},
  abstract = {Sympathetic nerve activity (SNA) can provide critical information on cardiovascular regulation; however, in a typical laboratory setting, adequate recordings require assiduous effort, and otherwise high-quality recordings may be clouded by frequent baseline shifts, noise spikes, and muscle twitches. Visually analyzing this type of signal can be a tedious and subjective evaluation, whereas objective analysis through signal averaging is impossible. We propose a new automated technique to identify bursts through objective detection criteria, eliminating artifacts and preserving a beat-by-beat SNA signal for a variety of subsequent analyses. The technique was evaluated during both steady-state conditions (17 subjects) and dynamic changes with rapid vasoactive drug infusion (14 recordings from 5 subjects) on SNA signals of widely varied quality. Automated measures of SNA were highly correlated to visual measures of steady-state activity ( r = 0.903, P {$<$} 0.001), dynamic relation measures ( r= 0.987, P {$<$} 0.001), and measures of burst-by-burst variability ( r = 0.929, P {$<$} 0.001). This automated sympathetic neurogram analysis provides a viable alternative to tedious and subjective visual analyses while maximizing the usability of noisy nerve tracings.},
  keywords = {Baroreflex,Microneurography,Time-series analysis},
  file = {/Users/asshah4/Library/CloudStorage/OneDrive-UniversityofIllinoisatChicago/articles/Hamner_Taylor_2001_Automated quantification of sympathetic beat-by-beat activity, independent of.pdf}
}

@article{Hannun2019,
  title = {Cardiologist-Level Arrhythmia Detection and Classification in Ambulatory Electrocardiograms Using a Deep Neural Network},
  author = {Hannun, Awni Y. and Rajpurkar, Pranav and Haghpanahi, Masoumeh and Tison, Geoffrey H. and Bourn, Codie and Turakhia, Mintu P. and Ng, Andrew Y.},
  date = {2019-01},
  journaltitle = {Nature Medicine},
  shortjournal = {Nat Med},
  volume = {25},
  number = {1},
  pages = {65--69},
  publisher = {{Nature Publishing Group}},
  issn = {1546-170X},
  doi = {10.1038/s41591-018-0268-3},
  url = {https://www.nature.com/articles/s41591-018-0268-3},
  urldate = {2023-07-24},
  abstract = {Computerized electrocardiogram (ECG) interpretation plays a critical role in the clinical ECG workflow1. Widely available digital ECG data and the algorithmic paradigm of deep learning2 present an opportunity to substantially improve the accuracy and scalability of automated ECG analysis. However, a comprehensive evaluation of an end-to-end deep learning approach for ECG analysis across a wide variety of diagnostic classes has not been previously reported. Here, we develop a deep neural network (DNN) to classify 12 rhythm classes using 91,232 single-lead ECGs from 53,549 patients who used a single-lead ambulatory ECG monitoring device. When validated against an independent test dataset annotated by a consensus committee of board-certified practicing cardiologists, the DNN achieved an average area under the receiver operating characteristic curve (ROC) of 0.97. The average F1 score, which is the harmonic mean of the positive predictive value and sensitivity, for the DNN (0.837) exceeded that of average cardiologists (0.780). With specificity fixed at the average specificity achieved by cardiologists, the sensitivity of the DNN exceeded the average cardiologist sensitivity for all rhythm classes. These findings demonstrate that an end-to-end deep learning approach can classify a broad range of distinct arrhythmias from single-lead ECGs with high diagnostic performance similar to that of cardiologists. If confirmed in clinical settings, this approach could reduce the rate of misdiagnosed computerized ECG interpretations and improve the efficiency of expert human ECG interpretation by accurately triaging or prioritizing the most urgent conditions.},
  issue = {1},
  langid = {english},
  keywords = {Arrhythmias,Machine learning},
  file = {/Users/asshah4/Library/CloudStorage/OneDrive-UniversityofIllinoisatChicago/articles/Hannun et al_2019_Cardiologist-level arrhythmia detection and classification in ambulatory.pdf}
}

@article{Heinze2001,
  title = {A Solution to the Problem of Monotone Likelihood in {{Cox}} Regression},
  author = {Heinze, Georg and Schemper, Michael},
  date = {2001},
  journaltitle = {Biometrics},
  volume = {57},
  number = {1},
  eprint = {11252585},
  eprinttype = {pmid},
  pages = {114--119},
  issn = {0006341X},
  doi = {10.1111/j.0006-341X.2001.00114.x},
  abstract = {The phenomenon of monotone likelihood is observed in the fitting process of a Cox model if the likelihood converges to a finite value while at least once parameter estimate diverges to ±∞. Monotone likelihood primarily occurs in small samples with substantial censoring of survival times and several highly predictive covariates. Previous options to deal with monotone likelihood have been unsatisfactory. The solution we suggest is an adaptation of a procedure by Firth (1993, Biometrika 80, 27-38) originally developed to reduce the bias of maximum likelihood estimates. This procedure produces finite parameter estimates by means of penalized maximum likelihood estimation. Corresponding Wald-type tests and confidence intervals are available, but it is shown that penalized likelihood ratio tests and profile penalized likelihood confidence intervals are often preferable. An empirical study of the suggested procedures confirms satisfactory performance of both estimation and inference. The advantage of the procedure over previous options of analysis is finally exemplified in the analysis of a breast cancer study.},
  keywords = {Bias reduction,Infinite estimates,Modified score,Penalized likelihood,Profile likelihood,Proportional hazards model,Separation,Survival analysis},
  file = {/Users/asshah4/Library/CloudStorage/OneDrive-UniversityofIllinoisatChicago/articles/Heinze_Schemper_2001_A solution to the problem of monotone likelihood in Cox regression.pdf}
}

@article{Hurst2005a,
  title = {Thoughts about the Ventricular Gradient and Its Current Clinical Use},
  author = {Hurst, J. Willis},
  date = {2005},
  journaltitle = {Clinical Cardiology},
  volume = {28},
  number = {5},
  eprint = {15971455},
  eprinttype = {pmid},
  pages = {219--224},
  issn = {01609289},
  doi = {10.1002/clc.4960280504},
  abstract = {The concept of the ventricular gradient was conceived in the mind of Frank Wilson in the early 1930s. Wilson, a mathematical genius, believed that the calculation of the ventricular gradient yielded information that was not otherwise obtainable. The method of analysis was not utilized by clinicians at large because the concept was not easy to understand and because the method used to compute the direction of the ventricular gradient was so time consuming that clinicians could not use it. Grant utilized the concept to create vector electrocardiography, but he believed that if his method of analysis was used, it was not often necessary to compute the direction of the ventricular gradient. He did, however, describe an easy way to compute the direction of the ventricular gradient. The current major clinical use of the ventricular gradient is to identify primary and secondary T-wave abnormalities in an electrocardiogram showing left or right ventricular hypertrophy or left or right ventricular conduction abnormalities. In addition, the author uses the term ventricular time gradient instead of ventricular gradient in an effort to clarify the concept. Finally, the author discusses the possible clinical significance of a normally directed, but shorter than normal, ventricular time gradient, an attribute that has not been emphasized previously.},
  isbn = {0160-9289 (Print)\textbackslash r0160-9289 (Linking)},
  keywords = {Ventricular time gradient},
  file = {/Users/asshah4/Library/CloudStorage/OneDrive-UniversityofIllinoisatChicago/articles/Hurst_2005_Thoughts about the ventricular gradient and its current clinical use.pdf;/Users/asshah4/Library/CloudStorage/OneDrive-UniversityofIllinoisatChicago/articles/Hurst_2005_Thoughts about the ventricular gradient and its current clinical use2.pdf}
}

@article{Jimenez-Perez2021,
  title = {Delineation of the Electrocardiogram with a Mixed-Quality-Annotations Dataset Using Convolutional Neural Networks},
  author = {Jimenez-Perez, Guillermo and Alcaine, Alejandro and Camara, Oscar},
  date = {2021-01-13},
  journaltitle = {Scientific Reports},
  shortjournal = {Sci Rep},
  volume = {11},
  number = {1},
  pages = {863},
  publisher = {{Nature Publishing Group}},
  issn = {2045-2322},
  doi = {10.1038/s41598-020-79512-7},
  url = {https://www.nature.com/articles/s41598-020-79512-7},
  urldate = {2023-08-02},
  abstract = {Detection and delineation are key steps for retrieving and structuring information of the electrocardiogram (ECG), being thus crucial for numerous tasks in clinical practice. Digital signal processing (DSP) algorithms are often considered state-of-the-art for this purpose but require laborious rule readaptation for adapting to unseen morphologies. This work explores the adaptation of the the U-Net, a deep learning (DL) network employed for image segmentation, to electrocardiographic data. The model was trained using PhysioNet’s QT database, a small dataset of 105 2-lead ambulatory recordings, while being independently tested for many architectural variations, comprising changes in the model’s capacity (depth, width) and inference strategy (single- and multi-lead) in a fivefold cross-validation manner. This work features several regularization techniques to alleviate data scarcity, such as semi-supervised pre-training with low-quality data labels, performing ECG-based data augmentation and applying in-built model regularizers. The best performing configuration reached precisions of 90.12\%, 99.14\% and 98.25\% and recalls of 98.73\%, 99.94\% and 99.88\% for the P, QRS and T waves, respectively, on par with DSP-based approaches. Despite being a data-hungry technique trained on a small dataset, a U-Net based approach demonstrates to be a viable alternative for this task.},
  issue = {1},
  langid = {english},
  keywords = {Biomedical engineering,Computer science},
  file = {/Users/asshah4/Library/CloudStorage/OneDrive-UniversityofIllinoisatChicago/articles/Jimenez-Perez et al_2021_Delineation of the electrocardiogram with a mixed-quality-annotations dataset.pdf}
}

@article{Kabir2017,
  title = {Optimal Configuration of Adhesive {{ECG}} Patches Suitable for Long-Term Monitoring of a Vectorcardiogram},
  author = {Kabir, Muammar M and Perez-Alday, Erick A and Thomas, Jason and Sedaghat, Golriz and Tereshchenko, Larisa G},
  date = {2017},
  journaltitle = {Journal of Electrocardiology},
  volume = {50},
  number = {3},
  eprint = {28069275},
  eprinttype = {pmid},
  pages = {342--348},
  publisher = {{NIH Public Access}},
  issn = {15328430},
  doi = {10.1016/j.jelectrocard.2016.12.005},
  url = {http://www.ncbi.nlm.nih.gov/pubmed/28069275},
  urldate = {2019-10-09},
  abstract = {The purpose of this study was to develop optimal configuration of adhesive ECG patches placement on the torso, which would provide the best agreement with the Frank orthogonal ECGs. Ten seconds of orthogonal ECG followed by 3–5 min of ECGs using patches at 5 different locations simultaneously on the torso were recorded in 50 participants at rest in sitting position. Median beat was generated for each ECG and 3 patch ECGs that best correlate with orthogonal ECGs were selected for each participant. For agreement analysis, spatial QRS-T angle, spatial QRS and T vector characteristics, spatial ventricular gradient, roundness, thickness and planarity of vectorcardiographic (VCG) loops were measured. Key VCG parameters showed high agreement in Bland–Altman analysis (spatial QRS-T angle on 3-patch ECG vs. Frank ECG bias 0.3 (95\% limits of agreement [−6.23;5.71 degrees]), Lin's concordance coefficient = 0.996). In conclusion, newly developed orthogonal 3-patch ECG can be used for long-term VCG monitoring.},
  keywords = {ECG patch,QRS-T angle,Vectorcardiogram},
  file = {/Users/asshah4/Library/CloudStorage/OneDrive-UniversityofIllinoisatChicago/articles/Kabir et al_2017_Optimal configuration of adhesive ECG patches suitable for long-term monitoring.pdf}
}

@article{Kentta2018,
  title = {Repolarization {{Heterogeneity Measured}} with {{T-Wave Area Dispersion}} in {{Standard}} 12-{{Lead ECG Predicts Sudden Cardiac Death}} in {{General Population}}},
  author = {Kenttä, Tuomas V. and Sinner, Moritz F. and Nearing, Bruce D. and Freudling, Rebecca and Porthan, Kimmo and Tikkanen, Jani T. and Müller-Nurasyid, Martina and Schramm, Katharina and Viitasalo, Matti and Jula, Antti and Nieminen, Markku S. and Peters, Annette and Salomaa, Veikko and Oikarinen, Lasse and Verrier, Richard L. and Kääb, Stefan and Junttila, M. Juhani and Huikuri, Heikki V.},
  date = {2018-02-01},
  journaltitle = {Circulation: Arrhythmia and Electrophysiology},
  volume = {11},
  number = {2},
  eprint = {29440187},
  eprinttype = {pmid},
  publisher = {{Lippincott Williams and Wilkins}},
  issn = {19413084},
  doi = {10.1161/CIRCEP.117.005762},
  url = {www.qtdrugs.org.},
  urldate = {2021-02-07},
  abstract = {Background: We developed a novel electrocardiographic marker, T-wave area dispersion (TW-Ad), which measures repolarization heterogeneity by assessing interlead T-wave areas during a single cardiac cycle and tested whether it can identify patients at risk for sudden cardiac death (SCD) in the general population. Methods and Results: TW-Ad was measured from standard digital 12-lead ECG in 5618 adults (46\% men; age, 50.9±12.5 years) participating in the Health 2000 Study - an epidemiological survey representative of the Finnish adult population. Independent replication was performed in 3831 participants of the KORA S4 Study (Cooperative Health Research in the Region of Augsburg; 49\% men; age, 48.7±13.7 years; mean follow-up, 8.8±1.1 years). During follow-up (7.7±1.4 years), 72 SCDs occurred in the Health 2000 Survey. Lower TW-Ad was univariately associated with SCD (0.32±0.36 versus 0.60±0.19; P{$<$}0.001); it had an area under the receiver operating characteristic curve of 0.809. TW-Ad (≤0.46) conferred a hazard ratio of 10.8 (95\% confidence interval, 6.8-17.4; P{$<$}0.001) for SCD; it remained independently predictive of SCD after multivariable adjustment for clinical risk markers (hazard ratio, 4.6; 95\% confidence interval, 2.7-7.4; P{$<$}0.001). Replication analyses performed in the KORA S4 Study confirmed an increased risk for cardiac death (unadjusted hazard ratio, 5.5; 95\% confidence interval, 3.2-9.5; P{$<$}0.001; multivariable adjusted hazard ratio, 1.9; 95\% confidence interval, 1.1-3.5; P{$<$}0.05). Conclusion: Low TW-Ad, reflecting increased heterogeneity of repolarization, in standard 12-lead resting ECGs is a powerful and independent predictor of SCD in the adult general population.},
  keywords = {confidence intervals,electrocardiography,heart,male,risk}
}

@article{Kleiger2005b,
  title = {Heart Rate Variability: {{Measurement}} and Clinical Utility},
  author = {Kleiger, Robert E. and Stein, Phyllis K. and Bigger, J. Thomas},
  date = {2005-01-01},
  journaltitle = {Annals of Noninvasive Electrocardiology},
  volume = {10},
  number = {1},
  eprint = {15649244},
  eprinttype = {pmid},
  pages = {88--101},
  publisher = {{Wiley/Blackwell (10.1111)}},
  issn = {1082-720X},
  doi = {10.1111/j.1542-474X.2005.10101.x},
  url = {http://doi.wiley.com/10.1111/j.1542-474X.2005.10101.x},
  urldate = {2018-11-13},
  abstract = {Electrocardiographic RR intervals fluctuate cyclically, modulated by ventilation, baroreflexes, and other genetic and environmental factors that are mediated through the autonomic nervous system. Short term electrocardiographic recordings (5 to 15 minutes), made under controlled conditions, e.g., lying supine or standing or tilted upright can elucidate physiologic, pharmacologic, or pathologic changes in autonomic nervous system function. Long-term, usually 24-hour recordings, can be used to assess autonomic nervous responses during normal daily activities in health, disease, and in response to therapeutic interventions, e.g., exercise or drugs. RR interval variability is useful for assessing risk of cardiovascular death or arrhythmic events, especially when combined with other tests, e.g., left ventricular ejection fraction or ventricular arrhythmias.},
  isbn = {1542-474X},
  keywords = {Autonomic nervous system},
  file = {/Users/asshah4/Library/CloudStorage/OneDrive-UniversityofIllinoisatChicago/articles/Kleiger et al_2005_Heart rate variability.pdf;/Users/asshah4/Library/CloudStorage/OneDrive-UniversityofIllinoisatChicago/articles/Kleiger et al_2005_Heart rate variability2.pdf}
}

@article{Leur2020,
  title = {Big {{Data}} and {{Artificial Intelligence}}: {{Opportunities}} and {{Threats}} in {{Electrophysiology}}},
  shorttitle = {Big {{Data}} and {{Artificial Intelligence}}},
  author = {family=Leur, given=Rutger R., prefix=van de, useprefix=false},
  date = {2020-03-08},
  url = {https://www.aerjournal.com/articles/big-data-and-artificial-intelligence-opportunities-and-threats-electrophysiology},
  urldate = {2023-07-30},
  abstract = {The combination of big data and artificial intelligence (AI) is having an increasing impact on the field of electrophysiology. Algorithms are created to improve the automated diagnosis of},
  langid = {english},
  file = {/Users/asshah4/Library/CloudStorage/OneDrive-UniversityofIllinoisatChicago/articles/Leur_2020_Big Data and Artificial Intelligence.pdf}
}

@article{Lewkowicz2002,
  title = {Description of Complex Time Series by Multipoles},
  author = {Lewkowicz, M. and Levitan, J. and Puzanov, N. and Shnerb, N. and Saermark, K.},
  date = {2002},
  journaltitle = {Physica A: Statistical Mechanics and its Applications},
  volume = {311},
  number = {1-2},
  pages = {260--274},
  issn = {03784371},
  doi = {10.1016/S0378-4371(02)00831-2},
  abstract = {We present a new method to describe time series with a highly complex time evolution. The time series is projected onto a two-dimensional phase-space plot which is quantified in terms of a multipole expansion where every data point is assigned a unit mass. The multipoles provide an efficient characterization of the original time series. © 2002 Elsevier Science B.V. All rights reserved.},
  keywords = {Heart rate variability,Non-stationary time series,Time series analysis},
  file = {/Users/asshah4/Library/CloudStorage/OneDrive-UniversityofIllinoisatChicago/articles/Lewkowicz et al_2002_Description of complex time series by multipoles.pdf}
}

@article{Liao1996,
  title = {A Computer Algorithm to Impute Interrupted Heart Rate Data for the Spectral Analysis of Heart Rate Variability - {{The ARIC}} Study},
  author = {Liao, Duanping and Barnes, Ralph W. and Chambless, Lloyd E. and Heiss, Gerardo},
  date = {1996},
  journaltitle = {Computers and Biomedical Research},
  volume = {29},
  number = {2},
  pages = {140--151},
  issn = {00104809},
  doi = {10.1006/cbmr.1996.0012},
  abstract = {The shorter term beat-to-beat heart rate data collected from the general population are often interrupted by artifacts, and an arbitrary exclusion of such individuals from analysis may significantly reduce the sample size and/or introduce selection bias. A computer algorithm was developed to label as artifacts any data points outside the upper and lower limits generated by a 5-beat moving average ±25\% (or set manually by an operator using a mouse) and to impute beat-to-beat heart rate throughout an artifact period to preserve the timing relationships of the adjacent, uncorrupted heart rate data. The algorithm applies Fast Fourier Transformation to the smoothed data to estimate low-frequency (LF; 0.025-0.15 Hz) and high-frequency (HF; 0.16-0.35 Hz) spectral powers and the HF/LF ratio as conventional indices of sympathetic, vagal, and vagal-sympathetic balance components, respectively. We applied this algorithm to resting, supine, 2-min beat-to-beat heart rate data collected in the population-based Atherosclerosis Risk in Communities study to assess the performance (success rate) of the algorithm (N = 526) and the inter- and intra-data-operator repeatability of using this computer algorithm (N = 108). Eighty-eight percent (88\%) of the records could be smoothed by the computer-generated limits, an additional 4.8\% by manually set limits, and 7.4\% of the data could not be processed due to a large number of artifacts in the beginning or the end of the records. For the repeatability study, 108 records were selected at random, and two trained data operators applied this algorithm to the same records twice within a 6-month interval of each process (blinded to each other's results and their own prior results). The inter-data-operator reliability coefficients were 0.86, 0.92, and 0.90 for the HF, LF, and HF/LF components, respectively. The average intra-data-operator reliability coefficients were 0.99, 0.99, and 0.98 for the HF, LF, and HF/LF components, respectively. These results indicate that this computer algorithm is efficient and highly repeatable in processing short-term beat-to-beat heart rate data collected from the general population, given that the data operators are trained according to standardized protocol.},
  file = {/Users/asshah4/Library/CloudStorage/OneDrive-UniversityofIllinoisatChicago/articles/Liao et al_1996_A computer algorithm to impute interrupted heart rate data for the spectral.pdf}
}

@article{Medvedovsky2020,
  title = {Prognostic Significance of the Frontal {{QRS-T}} Angle in Patients with {{AL}} Cardiac Amyloidosis},
  author = {Medvedovsky, Anna Turyan and Pollak, Arthur and Shuvy, Mony and Gotsman, Israel},
  date = {2020},
  journaltitle = {Journal of Electrocardiology},
  volume = {59},
  eprint = {32062381},
  eprinttype = {pmid},
  pages = {122--125},
  issn = {15328430},
  doi = {10.1016/j.jelectrocard.2020.02.001},
  url = {https://doi.org/10.1016/j.jelectrocard.2020.02.001},
  urldate = {2020-06-04},
  abstract = {Introduction: Cardiac involvement is a leading cause of morbidity and mortality in primary light chain (AL) amyloidosis. The electrocardiographic spatial QRS-T angle reflects changes in the direction of the repolarization sequence and is a powerful predictor of outcome in patients with heart failure. We examined the significance of the frontal QRS-T angle in predicting the clinical outcome in patients with AL cardiac amyloidosis. Methods: Forty-three consecutive patients with cardiac involvement of AL amyloidosis were studied. Patients were followed for survival. Results: Patient median age was 62 years, 56\% were males. After a median follow up of 56 months, 16 out of 43 patients had died (37\%). The median QRS-T angle was 102° (interquartile range 35–148). QRS-T angle{$>$}102° was associated with increased prevalence of lambda free light chain disease and the presence of a pleural effusion. It was also associated with increased interventricular septum thickness, smaller left ventricle end-diastolic diameter, echocardiographic myocardial sparkling texture, pericardial effusion, elevated NT-Pro-BNP and increased restrictive physiology evident by increased E/A and E/e`. A QRS-T angle{$>$}102° was a significant predictor of increased mortality by Kaplan-Meier survival analysis (71.6 ± 11.1\% vs. 45.7 ± 11.1\%, P = .02). A QRS-T angle{$>$}102° was an independent predictor of mortality by Cox regression analysis (HR 3.00, 95\% CI 1.01–8.89, P {$<$} .05). Conclusions: The QRS-T angle is associated with indices of advanced amyloid disease and is an independent predictor of survival.},
  keywords = {Amyloidosis,Outcome,QRS-T angle},
  file = {/Users/asshah4/Library/CloudStorage/OneDrive-UniversityofIllinoisatChicago/articles/Medvedovsky et al_2020_Prognostic significance of the frontal QRS-T angle in patients with AL cardiac.pdf}
}

@inproceedings{Moskalenko2020,
  title = {Deep {{Learning}} for {{ECG Segmentation}}},
  booktitle = {Advances in {{Neural Computation}}, {{Machine Learning}}, and {{Cognitive Research III}}},
  author = {Moskalenko, Viktor and Zolotykh, Nikolai and Osipov, Grigory},
  editor = {Kryzhanovsky, Boris and Dunin-Barkowski, Witali and Redko, Vladimir and Tiumentsev, Yury},
  date = {2020},
  series = {Studies in {{Computational Intelligence}}},
  pages = {246--254},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-30425-6_29},
  abstract = {We propose an algorithm for electrocardiogram (ECG) segmentation using a UNet-like full-convolutional neural network. The algorithm receives an arbitrary sampling rate ECG signal as an input, and gives a list of onsets and offsets of P and T waves and QRS complexes as output. Our method of segmentation differs from others in speed, a small number of parameters and a good generalization: it is adaptive to different sampling rates and it is generalized to various types of ECG monitors. The proposed approach is superior to other state-of-the-art segmentation methods in terms of quality. In particular, F1-measures for detection of onsets and offsets of P and T waves and for QRS-complexes are at least 97.8\%, 99.5\%, and 99.9\%, respectively.},
  isbn = {978-3-030-30425-6},
  langid = {english},
  keywords = {ECG delineation,ECG segmentation,Electrocardiography,UNet},
  file = {/Users/asshah4/Library/CloudStorage/OneDrive-UniversityofIllinoisatChicago/articles/Moskalenko et al_2020_Deep Learning for ECG Segmentation.pdf}
}

@article{Nagamine2020,
  title = {Multiscale Classification of Heart Failure Phenotypes by Unsupervised Clustering of Unstructured Electronic Medical Record Data},
  author = {Nagamine, Tasha and Gillette, Brian and Pakhomov, Alexey and Kahoun, John and Mayer, Hannah and Burghaus, Rolf and Lippert, Jörg and Saxena, Mayur},
  date = {2020-12-07},
  journaltitle = {Scientific Reports},
  shortjournal = {Sci Rep},
  volume = {10},
  number = {1},
  pages = {21340},
  publisher = {{Nature Publishing Group}},
  issn = {2045-2322},
  doi = {10.1038/s41598-020-77286-6},
  url = {https://www.nature.com/articles/s41598-020-77286-6},
  urldate = {2023-07-27},
  abstract = {As a leading cause of death and morbidity, heart failure (HF) is responsible for a large portion of healthcare and disability costs worldwide. Current approaches to define specific HF subpopulations may fail to account for the diversity of etiologies, comorbidities, and factors driving disease progression, and therefore~have limited value for clinical decision making and development of novel therapies. Here we present a novel and data-driven approach to understand and characterize the real-world manifestation of HF by clustering disease and symptom-related clinical concepts (complaints) captured from unstructured electronic health record clinical notes. We used natural language processing to construct vectorized representations of patient complaints followed by clustering to group HF patients by similarity of complaint vectors. We then identified complaints that were significantly enriched within each cluster using statistical testing. Breaking the HF population into groups of similar patients revealed a clinically interpretable hierarchy of subgroups characterized by similar HF manifestation. Importantly, our methodology revealed well-known etiologies, risk factors, and comorbid conditions of HF (including ischemic heart disease, aortic valve disease, atrial fibrillation, congenital heart disease, various cardiomyopathies, obesity, hypertension, diabetes, and chronic kidney disease) and yielded additional insights into the details of each HF subgroup’s clinical manifestation of HF. Our approach is entirely hypothesis free and can therefore be readily applied for discovery of novel insights in alternative diseases or patient populations.},
  issue = {1},
  langid = {english},
  keywords = {Heart failure,Machine learning},
  file = {/Users/asshah4/Library/CloudStorage/OneDrive-UniversityofIllinoisatChicago/articles/Nagamine et al_2020_Multiscale classification of heart failure phenotypes by unsupervised.pdf}
}

@article{Nagamine2022,
  title = {Data-Driven Identification of Heart Failure Disease States and Progression Pathways Using Electronic Health Records},
  author = {Nagamine, Tasha and Gillette, Brian and Kahoun, John and Burghaus, Rolf and Lippert, Jörg and Saxena, Mayur},
  date = {2022-10-25},
  journaltitle = {Scientific Reports},
  shortjournal = {Sci Rep},
  volume = {12},
  number = {1},
  pages = {17871},
  publisher = {{Nature Publishing Group}},
  issn = {2045-2322},
  doi = {10.1038/s41598-022-22398-4},
  url = {https://www.nature.com/articles/s41598-022-22398-4},
  urldate = {2023-07-27},
  abstract = {Heart failure (HF) is a leading cause of morbidity, healthcare costs, and mortality. Guideline based segmentation of HF into distinct subtypes is coarse and unlikely to reflect the heterogeneity of etiologies and disease trajectories of patients. While analyses of electronic health records show promise in expanding our understanding of complex syndromes like HF in an evidence-driven way, limitations in data quality have presented challenges for large-scale EHR-based insight generation and decision-making. We present a hypothesis-free approach to generating real-world characteristics and progression patterns of HF. Patient disease state snapshots are extracted from the complaints mentioned in unstructured clinical notes. Typical disease states are generated by clustering and characterized in terms of their distinguishing features, temporal relationships, and risk of important clinical events. Our analysis generates a comprehensive “disease phenome” of real-world patients computed from large, noisy, secondary-use EHR datasets created in a routine clinical setting.},
  issue = {1},
  langid = {english},
  keywords = {Computational models,Heart failure},
  file = {/Users/asshah4/Library/CloudStorage/OneDrive-UniversityofIllinoisatChicago/articles/Nagamine et al_2022_Data-driven identification of heart failure disease states and progression.pdf}
}

@article{Nagashima2017,
  title = {Information Criteria for {{Firth}}'s Penalized Partial Likelihood Approach in {{Cox}} Regression Models},
  author = {Nagashima, Kengo and Sato, Yasunori},
  date = {2017-09-20},
  journaltitle = {Statistics in Medicine},
  volume = {36},
  number = {21},
  eprint = {28608396},
  eprinttype = {pmid},
  pages = {3422--3436},
  publisher = {{John Wiley \& Sons, Ltd}},
  issn = {10970258},
  doi = {10.1002/sim.7368},
  url = {https://onlinelibrary-wiley-com.proxy.cc.uic.edu/doi/full/10.1002/sim.7368},
  urldate = {2022-09-21},
  abstract = {In the estimation of Cox regression models, maximum partial likelihood estimates might be infinite in a monotone likelihood setting, where partial likelihood converges to a finite value and parameter estimates converge to infinite values. To address monotone likelihood, previous studies have applied Firth's bias correction method to Cox regression models. However, while the model selection criteria for Firth's penalized partial likelihood approach have not yet been studied, a heuristic AIC-type information criterion can be used in a statistical package. Application of the heuristic information criterion to data obtained from a prospective observational study of patients with multiple brain metastases indicated that the heuristic information criterion selects models with many parameters and ignores the adequacy of the model. Moreover, we showed that the heuristic information criterion tends to select models with many regression parameters as the sample size increases. Thereby, in the present study, we propose an alternative AIC-type information criterion based on the risk function. A Bayesian information criterion type was also evaluated. Further, the presented simulation results confirm that the proposed criteria performed well in a monotone likelihood setting. The proposed AIC-type criterion was applied to prospective observational study data. Copyright © 2017 John Wiley \& Sons, Ltd.},
  keywords = {Akaike's information criterion,model selection,monotone likelihood,penalized partial likelihood,survival analysis},
  file = {/Users/asshah4/Library/CloudStorage/OneDrive-UniversityofIllinoisatChicago/articles/Nagashima_Sato_2017_Information criteria for Firth's penalized partial likelihood approach in Cox.pdf}
}

@article{Nair2012,
  title = {{{CARRS Surveillance}} Study: {{Design}} and Methods to Assess Burdens from Multiple Perspectives},
  author = {Nair, Manisha and Ali, Mohammed K. and Ajay, Vamadevan S. and Shivashankar, Roopa and Mohan, Viswanathan and Pradeepa, Rajendra and Deepa, Mohan and Khan, Hassan M. and Kadir, Muhammad M. and Fatmi, Zafar A. and Reddy, K. Srinath and Tandon, Nikhil and Narayan, K. M.Venkat and Prabhakaran, Dorairaj},
  date = {2012},
  journaltitle = {BMC Public Health},
  volume = {12},
  number = {1},
  pages = {1},
  publisher = {{BMC Public Health}},
  issn = {14712458},
  doi = {10.1186/1471-2458-12-701},
  url = {BMC Public Health},
  abstract = {Background: Cardio-metabolic diseases (CMDs) are a growing public health problem, but data on incidence, trends, and costs in developing countries is scarce. Comprehensive and standardised surveillance for non-communicable diseases was recommended at the United Nations High-level meeting in 2011. Aims: To develop a model surveillance system for CMDs and risk factors that could be adopted for continued assessment of burdens from multiple perspectives in South-Asian countries. Methods. Design: Hybrid model with two cross-sectional serial surveys three years apart to monitor trend, with a three-year prospective follow-up of the first cohort. Sites: Three urban settings (Chennai and New Delhi in India; Karachi in Pakistan), 4000 participants in each site stratified by gender and age. Sampling methodology: Multi-stage cluster random sampling; followed by within-household participant selection through a combination of Health Information National Trends Study (HINTS) and Kish methods. Culturally-appropriate and methodologically-relevant data collection instruments were developed to gather information on CMDs and their risk factors; quality of life, health-care utilisation and costs, along with objective measures of anthropometric, clinical and biochemical parameters. The cohort follow-up is designed as a pilot study to understand the feasibility of estimating incidence of risk factors, disease events, morbidity, and mortality. Results: The overall participant response rate in the first cross-sectional survey was 94.1\% (Chennai 92.4\%, n=4943; Delhi 95.7\%, n=4425; Karachi 94.3\%, n=4016). 51.8\% of the participants were females, 61.6\%{$<$}45years, 27.5\% 45-60years and 10.9\% {$>$}60years. Discussion. This surveillance model will generate data on prevalence and trends; help study the complex life-course patterns of CMDs, and provide a platform for developing and testing interventions and tools for prevention and control of CMDs in South-Asia. It will also help understanding the challenges and opportunities in establishing a surveillance system across countries. © 2012 Nair et al.; licensee BioMed Central Ltd.},
  keywords = {Cardio-metabolic diseases,Risk-factors,South-Asia,Surveillance},
  file = {/Users/asshah4/Library/CloudStorage/OneDrive-UniversityofIllinoisatChicago/articles/Nair et al_2012_CARRS Surveillance study.pdf}
}

@article{Nemati2018,
  title = {An {{Interpretable Machine Learning Model}} for {{Accurate Prediction}} of {{Sepsis}} in the {{ICU}}},
  author = {Nemati, Shamim and Holder, Andre and Razmi, Fereshteh and Stanley, Matthew D and Clifford, Gari D and Buchman, Timothy G},
  date = {2018},
  journaltitle = {Critical care medicine},
  volume = {46},
  number = {4},
  pages = {547--553},
  issn = {15300293},
  doi = {10.1097/CCM.0000000000002936},
  url = {www.sc2i.org,},
  urldate = {2020-04-17},
  abstract = {OBJECTIVES: Sepsis is among the leading causes of morbidity, mortality, and cost overruns in critically ill patients. Early intervention with antibiotics improves survival in septic patients. However, no clinically validated system exists for real-time prediction of sepsis onset. We aimed to develop and validate an Artificial Intelligence Sepsis Expert algorithm for early prediction of sepsis. DESIGN: Observational cohort study. SETTING: Academic medical center from January 2013 to December 2015. PATIENTS: Over 31,000 admissions to the ICUs at two Emory University hospitals (development cohort), in addition to over 52,000 ICU patients from the publicly available Medical Information Mart for Intensive Care-III ICU database (validation cohort). Patients who met the Third International Consensus Definitions for Sepsis (Sepsis-3) prior to or within 4 hours of their ICU admission were excluded, resulting in roughly 27,000 and 42,000 patients within our development and validation cohorts, respectively.None. MEASUREMENTS AND MAIN RESULTS: High-resolution vital signs time series and electronic medical record data were extracted. A set of 65 features (variables) were calculated on hourly basis and passed to the Artificial Intelligence Sepsis Expert algorithm to predict onset of sepsis in the proceeding T hours (where T = 12, 8, 6, or 4). Artificial Intelligence Sepsis Expert was used to predict onset of sepsis in the proceeding T hours and to produce a list of the most significant contributing factors. For the 12-, 8-, 6-, and 4-hour ahead prediction of sepsis, Artificial Intelligence Sepsis Expert achieved area under the receiver operating characteristic in the range of 0.83-0.85. Performance of the Artificial Intelligence Sepsis Expert on the development and validation cohorts was indistinguishable. CONCLUSIONS: Using data available in the ICU in real-time, Artificial Intelligence Sepsis Expert can accurately predict the onset of sepsis in an ICU patient 4-12 hours prior to clinical recognition. A prospective study is necessary to determine the clinical utility of the proposed sepsis prediction model.},
  keywords = {informatics,machine learning,organ failure,prognostication,Sepsis,SOFA},
  file = {/Users/asshah4/Library/CloudStorage/OneDrive-UniversityofIllinoisatChicago/articles/Nemati et al_2018_An Interpretable Machine Learning Model for Accurate Prediction of Sepsis in.pdf}
}

@article{Niroshana2023,
  title = {Beat-Wise Segmentation of Electrocardiogram Using Adaptive Windowing and Deep Neural Network},
  author = {Niroshana, S. M. Isuru and Kuroda, Satoshi and Tanaka, Kazuyuki and Chen, Wenxi},
  date = {2023-07-07},
  journaltitle = {Scientific Reports},
  shortjournal = {Sci Rep},
  volume = {13},
  number = {1},
  pages = {11039},
  publisher = {{Nature Publishing Group}},
  issn = {2045-2322},
  doi = {10.1038/s41598-023-37773-y},
  url = {https://www.nature.com/articles/s41598-023-37773-y},
  urldate = {2023-08-09},
  abstract = {Timely detection of anomalies and automatic interpretation of an electrocardiogram (ECG) play a crucial role in many healthcare applications, such as patient monitoring and post treatments. Beat-wise segmentation is one of the essential steps in ensuring the confidence and fidelity of many automatic ECG classification methods. In this sense, we present a reliable ECG beat segmentation technique using a CNN model with an adaptive windowing algorithm. The proposed adaptive windowing algorithm can recognise cardiac cycle events and perform segmentation, including regular and irregular beats from an ECG signal with satisfactorily accurate boundaries.The proposed algorithm was evaluated quantitatively and qualitatively based on the annotations provided with the datasets and beat-wise manual inspection. The algorithm performed satisfactorily well for the MIT-BIH dataset with a 99.08\% accuracy and a 99.08\% of F1-score in detecting heartbeats along with a 99.25\% of accuracy in determining correct boundaries. The proposed method successfully detected heartbeats from the European S-T database with a 98.3\% accuracy and 97.4\% precision. The algorithm showed 99.4\% of accuracy and precision for Fantasia database. In summary, the algorithm’s overall performance on these three datasets suggests a high possibility of applying this algorithm in various applications in ECG analysis, including clinical applications with greater confidence.},
  issue = {1},
  langid = {english},
  keywords = {Biomedical engineering,Cardiology,Health care,Health occupations,Medical research},
  file = {/Users/asshah4/Library/CloudStorage/OneDrive-UniversityofIllinoisatChicago/articles/Niroshana et al_2023_Beat-wise segmentation of electrocardiogram using adaptive windowing and deep.pdf}
}

@article{Oehler2014,
  title = {{{QRS-T Angle}}: {{A Review}}},
  author = {Oehler, Andrew and Feldman, Trevor and Henrikson, Charles A and Tereshchenko, Larisa G},
  date = {2014},
  journaltitle = {Annals of Noninvasive Electrocardiology},
  volume = {19},
  number = {6},
  pages = {534--542},
  issn = {1542474X},
  doi = {10.1111/anec.12206},
  abstract = {For proper distribution of preventative resources, a more robust method of cardiac risk stratification should be encouraged in addition to merely reduced ejection fraction. To this end, the QRS-T angle, an electrocardiogram-derived measure of the difference in mean vectors of depolarization and repolarization, has been found associated with sudden cardiac death and other mortal and morbid outcomes in multiple observational studies over the past decade. The use of both frontal and spatial QRS-T angle in the prediction of future cardiac events including sudden cardiac death, all-cause mortality, and further cardiac morbidity is reviewed here.},
  keywords = {cardiac arrest,clinical,electrocardiography,electrophysiology,noninvasive techniques,sudden death},
  file = {/Users/asshah4/Library/CloudStorage/OneDrive-UniversityofIllinoisatChicago/articles/Oehler et al_2014_QRS-T Angle.pdf}
}

@article{Olesen2005,
  title = {Statistical Analysis of the {{DIAMOND MI}} Study by the Multipole Method},
  author = {Olesen, R. M. and Bloch Thomsen, P. E. and Saermark, K. and Glikson, M. and Feldman, S. and Lewkowicz, M. and Levitan, J.},
  date = {2005-10-01},
  journaltitle = {Physiological Measurement},
  volume = {26},
  number = {5},
  eprint = {9226893},
  eprinttype = {pmid},
  pages = {591--598},
  issn = {09673334},
  doi = {10.1088/0967-3334/26/5/002},
  url = {http://stacks.iop.org/0967-3334/26/i=5/a=002?key=crossref.fae68af48013829ef154a6b703ade0b3},
  abstract = {Proliferative enteritis is an enteric disease that affects a variety of animals. The causative agent in swine has been determined to be an obligate intracellular bacterium, Lawsonia intracellularis, related to the sulfate-reducing bacterium Desulfovibrio desulfuricans. The intracellular agents found in the lesions of different animal species are antigenically similar. In addition, strains from the pig, ferret, and hamster have been shown to be genetically similar. In this study we performed a partial 16S ribosomal DNA sequence analysis on the intracellular agent of proliferative enteritis from a hamster, a deer, and an ostrich and compared these sequences to that of the porcine L. intracellularis isolate. Results of this study indicate that the intracellular agents from these species with proliferative enteritis have high sequence similarity, indicating that they are all in the genus Lawsonia and that they may also be the same species, L. intracellularis.},
  isbn = {0020-7713},
  keywords = {circadian,DIAMOND MI study,Heart rate variability,Multipoles,Recurrence plot},
  file = {/Users/asshah4/Library/CloudStorage/OneDrive-UniversityofIllinoisatChicago/articles/Olesen et al_2005_Statistical analysis of the DIAMOND MI study by the multipole method.pdf}
}

@article{Ozga2018a,
  title = {A Systematic Comparison of Recurrent Event Models for Application to Composite Endpoints},
  author = {Ozga, Ann Kathrin and Kieser, Meinhard and Rauch, Geraldine},
  date = {2018},
  journaltitle = {BMC Medical Research Methodology},
  volume = {18},
  number = {1},
  issn = {14712288},
  doi = {10.1186/s12874-017-0462-x},
  abstract = {Background Many clinical trials focus on the comparison of the treatment effect between two or more groups concerning a rarely occurring event. In this situation, showing a relevant effect with an acceptable power requires the observation of a large number of patients over a long period of time. For feasibility issues, it is therefore often considered to include several event types of interest, non-fatal or fatal, and to combine them within a composite endpoint. Commonly, a composite endpoint is analyzed with standard survival analysis techniques by assessing the time to the first occurring event. This approach neglects that an individual may experience more than one event which leads to a loss of information. As an alternative, composite endpoints could be analyzed by models for recurrent events. There exists a number of such models, e.g. regression models based on count data or Cox-based models such as the approaches of Andersen and Gill, Prentice, Williams and Peterson or, Wei, Lin and Weissfeld. Although some of the methods were already compared within the literature there exists no systematic investigation for the special requirements regarding composite endpoints. Methods Within this work a simulation-based comparison of recurrent event models applied to composite endpoints is provided for different realistic clinical trial scenarios. Results We demonstrate that the Andersen-Gill model and the Prentice- Williams-Petersen models show similar results under various data scenarios whereas the Wei-Lin-Weissfeld model delivers effect estimators which can considerably deviate under commonly met data scenarios. Conclusion Based on the conducted simulation study, this paper helps to understand the pros and cons of the investigated methods in the context of composite endpoints and provides therefore recommendations for an adequate statistical analysis strategy and a meaningful interpretation of results.},
  keywords = {Composite endpoints,Recurrent event analysis,Simulation study},
  file = {/Users/asshah4/Library/CloudStorage/OneDrive-UniversityofIllinoisatChicago/articles/Ozga et al_2018_A systematic comparison of recurrent event models for application to composite.pdf}
}

@article{Pastori2020,
  title = {Clinical Phenotypes of Atrial Fibrillation and Risk of Mortality: A Cluster Analysis},
  author = {Pastori, D and Antonucci, E and Milanese, A and Violi, F and Pignatelli, P and Palareti, G and Farcomeni, A},
  date = {2020-11-01},
  journaltitle = {European Heart Journal},
  volume = {41},
  issn = {0195-668X},
  doi = {10.1093/ehjci/ehaa946.2893},
  url = {https://academic.oup.com/eurheartj/article/doi/10.1093/ehjci/ehaa946.2893/6004272},
  issue = {Supplement\_2},
  file = {/Users/asshah4/Library/CloudStorage/OneDrive-UniversityofIllinoisatChicago/articles/Pastori et al_2020_Clinical phenotypes of atrial fibrillation and risk of mortality.pdf}
}

@article{Pencina2011,
  title = {Extensions of Net Reclassification Improvement Calculations to Measure Usefulness of New Biomarkers},
  author = {Pencina, Michael J. and D'Agostino, Ralph B. and Steyerberg, Ewout W.},
  date = {2011-01-15},
  journaltitle = {Statistics in Medicine},
  volume = {30},
  number = {1},
  eprint = {21204120},
  eprinttype = {pmid},
  pages = {11--21},
  issn = {02776715},
  doi = {10.1002/sim.4085},
  abstract = {Appropriate quantification of added usefulness offered by new markers included in risk prediction algorithms is a problem of active research and debate. Standard methods, including statistical significance and c statistic are useful but not sufficient. Net reclassification improvement (NRI) offers a simple intuitive way of quantifying improvement offered by new markers and has been gaining popularity among researchers. However, several aspects of the NRI have not been studied in sufficient detail.In this paper we propose a prospective formulation for the NRI which offers immediate application to survival and competing risk data as well as allows for easy weighting with observed or perceived costs. We address the issue of the number and choice of categories and their impact on NRI. We contrast category-based NRI with one which is category-free and conclude that NRIs cannot be compared across studies unless they are defined in the same manner. We discuss the impact of differing event rates when models are applied to different samples or definitions of events and durations of follow-up vary between studies. We also show how NRI can be applied to case-control data. The concepts presented in the paper are illustrated in a Framingham Heart Study example.In conclusion, NRI can be readily calculated for survival, competing risk, and case-control data, is more objective and comparable across studies using the category-free version, and can include relative costs for classifications. We recommend that researchers clearly define and justify the choices they make when choosing NRI for their application. Copyright © 2010 John Wiley \& Sons, Ltd.},
  keywords = {Biomarker,Discrimination,Model performance,NRI,Risk prediction},
  file = {/Users/asshah4/Library/CloudStorage/OneDrive-UniversityofIllinoisatChicago/articles/Pencina et al_2011_Extensions of net reclassification improvement calculations to measure.pdf}
}

@article{Peng2022,
  title = {Clustering by Measuring Local Direction Centrality for Data with Heterogeneous Density and Weak Connectivity},
  author = {Peng, Dehua and Gui, Zhipeng and Wang, Dehe and Ma, Yuncheng and Huang, Zichen and Zhou, Yu and Wu, Huayi},
  date = {2022-09-16},
  journaltitle = {Nature Communications},
  shortjournal = {Nat Commun},
  volume = {13},
  number = {1},
  pages = {5455},
  publisher = {{Nature Publishing Group}},
  issn = {2041-1723},
  doi = {10.1038/s41467-022-33136-9},
  url = {https://www.nature.com/articles/s41467-022-33136-9},
  urldate = {2023-07-27},
  abstract = {Clustering is a powerful machine learning method for discovering similar patterns according to the proximity of elements in feature space. It is widely used in computer science, bioscience, geoscience, and economics. Although the state-of-the-art partition-based and connectivity-based clustering methods have been developed, weak connectivity and heterogeneous density in data impede their effectiveness. In this work, we propose a boundary-seeking Clustering algorithm using the local Direction Centrality (CDC). It adopts a density-independent metric based on the distribution of K-nearest neighbors (KNNs) to distinguish between internal and boundary points. The boundary points generate enclosed cages to bind the connections of internal points, thereby preventing cross-cluster connections and separating weakly-connected clusters. We demonstrate the validity of CDC by detecting complex structured clusters in challenging synthetic datasets, identifying cell types from single-cell RNA sequencing (scRNA-seq) and mass cytometry (CyTOF) data, recognizing speakers on voice corpuses, and testifying on various types of real-world benchmarks.},
  issue = {1},
  langid = {english},
  keywords = {Biomedical engineering,Classification and taxonomy,Computational models,Computer science,Data mining},
  file = {/Users/asshah4/Library/CloudStorage/OneDrive-UniversityofIllinoisatChicago/articles/Peng et al_2022_Clustering by measuring local direction centrality for data with heterogeneous.pdf}
}

@article{Perez-Alday2019,
  title = {Importance of the Heart Vector Origin Point Definition for an {{ECG}} Analysis: {{The Atherosclerosis Risk}} in {{Communities}} ({{ARIC}}) Study},
  author = {Perez-Alday, Erick Andres and Li-Pershing, Yin and Bender, Aron and Hamilton, Christopher and Thomas, Jason A. and Johnson, Kyle and Lee, Tiffany L. and Gonzales, Ryan and Li, Aaron and Newton, Kelley and Tereshchenko, Larisa G.},
  date = {2019-01-01},
  journaltitle = {Computers in Biology and Medicine},
  volume = {104},
  eprint = {30472495},
  eprinttype = {pmid},
  pages = {127--138},
  publisher = {{Elsevier Ltd}},
  issn = {18790534},
  doi = {10.1016/j.compbiomed.2018.11.013},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0010482518303755},
  urldate = {2020-11-12},
  abstract = {Aim: Our goal was to investigate the effect of a global XYZ median beat construction and the heart vector origin point definition on predictive accuracy of ECG biomarkers of sudden cardiac death (SCD). Methods: Atherosclerosis Risk In Community study participants with analyzable digital ECGs were included (n = 15,768; 55\% female, 73\% white, mean age 54.2 ± 5.8 y). We developed an algorithm to automatically detect the heart vector origin point on a median beat. Three different approaches to construct a global XYZ beat and two methods to locate origin point were compared. Global electrical heterogeneity was measured by sum absolute QRST integral (SAI QRST), spatial QRS-T angle, and spatial ventricular gradient (SVG) magnitude, azimuth, and elevation. Adjudicated SCD served as the primary outcome. Results: There was high intra-observer (kappa 0.972) and inter-observer (kappa 0.984) agreement in a heart vector origin definition between an automated algorithm and a human. QRS was wider in a median beat that was constructed using R-peak alignment than in time-coherent beat (88.1 ± 16.7 vs. 83.7 ± 15.9 ms; P {$<$} 0.0001), and on a median beat constructed using QRS-onset as a zeroed baseline, vs. isoelectric origin point (86.7 ± 15.9 vs. 83.7 ± 15.9 ms; P {$<$} 0.0001). ROC AUC was significantly larger for QRS, QT, peak QRS-T angle, SVG elevation, and SAI QRST if measured on a time-coherent median beat, and for SAI QRST and SVG magnitude if measured on a median beat using isoelectric origin point. Conclusion: Time-coherent global XYZ median beat with physiologically meaningful definition of the heart vector's origin point improved predictive accuracy of SCD biomarkers.},
  keywords = {Electrocardiography,Electrocardiology,Median beat,Origin point,Sudden cardiac death,Vectorcardiogram,Vectorcardiography},
  file = {/Users/asshah4/Library/CloudStorage/OneDrive-UniversityofIllinoisatChicago/articles/Perez-Alday et al_2019_Importance of the heart vector origin point definition for an ECG analysis.pdf}
}

@report{Racine2018,
  title = {A Primer on Regression Splines},
  author = {Racine, Jeffrey S},
  date = {2018},
  journaltitle = {CRAN. R-Project.},
  pages = {1--11},
  issn = {10944281},
  doi = {10.1177/1094428104272636},
  abstract = {1. Overview B-splines constitute an appealing method for the nonparametric estimation of a range of statis-tical objects of interest. In this primer we focus our attention on the estimation of a conditional mean, i.e. the 'regression function'. A 'spline' is a function that is constructed piece-wise from polynomial functions. The term comes from the tool used by shipbuilders and drafters to construct smooth shapes having desired properties. Drafters have long made use of a bendable strip fixed in position at a number of points that relaxes to form a smooth curve passing through those points. The malleability of the spline material combined with the constraint of the control points would cause the strip to take the shape that minimized the energy required for bending it between the fixed points, this being the smoothest possible shape. We shall rely on a class of splines called 'B-splines' ('basis-splines'). A B-spline function is the maximally differentiable interpolative basis function. The B-spline is a generalization of the Bézier curve (a B-spline with no 'interior knots' is a Bézier curve). B-splines are defined by their 'order' m and number of interior 'knots' N (there are two 'endpoints' which are themselves knots so the total number of knots will be N +2). The degree of the B-spline polynomial will be the spline order m minus one (degree = m − 1). To best appreciate the nature of B-splines, we shall first consider a simple type of spline, the Bézier function, and then move on to the more flexible and powerful generalization, the B-spline itself. We begin with the univariate case in Section 2 where we consider the univariate Bézier function. In Section 3 we turn to the univariate B-spline function, and then in Section 4 we turn to the multivariate case where we also briefly mention how one could handle the presence of categorical predictors. We presume that interest lies in 'regression spline' methodology which differs in a number of ways from 'smoothing splines', both of which are popular in applied settings. The fundamen-tal difference between the two approaches is that smoothing splines explicitly penalize roughness and use the data points themselves as potential knots whereas regression splines place knots at equidistant/equiquantile points. We direct the interested reader to Wahba (1990) for a treatment of smoothing splines. Date: December 18, 2014. These notes are culled from a variety of sources. I am solely responsible for all errors. Suggestions are welcomed (racinej@mcmaster.ca). 1 2 JEFFREY S. RACINE 2. Bézier curves We present an overview of Bézier curves which form the basis for the B-splines that follow. We begin with a simple illustration, that of a quadratic Bézier curve. Example 2.1. A quadratic Bézier curve. A quadratic Bézier curve is the path traced by the function B(x), given points β 0 , β 1 , and β 2 , where B(x) = β 0 (1 − x) 2 + 2β 1 (1 − x)x + β 2 x 2 = 2 i=0 β i B i (x), x ∈ [0, 1]. The terms B 0 (x) = (1 − x) 2 , B 1 (x) = 2(1 − x)x, and B 2 (x) = x 2 are the 'bases' which is this case turn out to be 'Bernstein polynomials' (Bernstein (1912)). For our purposes the 'control points' β i , i = 0, 1, 2, will be parameters that could be selected by least squares fitting in a regression setting, but more on that later. Consider the following simple example where we plot a quadratic Bézier curve with arbitrary control points: 0.0 0.2 0.4 0.6 0.8 1.0},
  isbn = {0031-5826},
  file = {/Users/asshah4/Library/CloudStorage/OneDrive-UniversityofIllinoisatChicago/articles/Racine_2018_A primer on regression splines.pdf}
}

@article{Raghupathi2014,
  title = {Big Data Analytics in Healthcare: Promise and Potential},
  author = {Raghupathi, Wullianallur and Raghupathi, Viju},
  date = {2014-12-07},
  journaltitle = {Health Information Science and Systems},
  volume = {2},
  number = {1},
  eprint = {25825667},
  eprinttype = {pmid},
  pages = {1--10},
  publisher = {{Springer Nature}},
  issn = {2047-2501},
  doi = {10.1186/2047-2501-2-3},
  abstract = {OBJECTIVE To describe the promise and potential of big data analytics in healthcare. METHODS The paper describes the nascent field of big data analytics in healthcare, discusses the benefits, outlines an architectural framework and methodology, describes examples reported in the literature, briefly discusses the challenges, and offers conclusions. RESULTS The paper provides a broad overview of big data analytics for healthcare researchers and practitioners. CONCLUSIONS Big data analytics in healthcare is evolving into a promising field for providing insight from very large data sets and improving outcomes while reducing costs. Its potential is great; however there remain challenges to overcome.},
  keywords = {Bioinformatics,Computational Biology/Bioinformatics,Health Informatics,Information Systems and Communication Service},
  file = {/Users/asshah4/Library/CloudStorage/OneDrive-UniversityofIllinoisatChicago/articles/Raghupathi_Raghupathi_2014_Big data analytics in healthcare.pdf}
}

@article{Rajendran2019,
  title = {Identification of Peripheral Neural Circuits That Regulate Heart Rate Using Optogenetic and Viral Vector Strategies},
  author = {Rajendran, Pradeep S. and Challis, Rosemary C. and Fowlkes, Charless C. and Hanna, Peter and Tompkins, John D. and Jordan, Maria C. and Hiyari, Sarah and Gabris-Weber, Beth A. and Greenbaum, Alon and Chan, Ken Y. and Deverman, Benjamin E. and Münzberg, Heike and Ardell, Jeffrey L. and Salama, Guy and Gradinaru, Viviana and Shivkumar, Kalyanam},
  date = {2019-04-26},
  journaltitle = {Nature Communications 2019 10:1},
  volume = {10},
  number = {1},
  eprint = {31028266},
  eprinttype = {pmid},
  pages = {1--13},
  publisher = {{Nature Publishing Group}},
  issn = {2041-1723},
  doi = {10.1038/s41467-019-09770-1},
  url = {https://www.nature.com/articles/s41467-019-09770-1},
  urldate = {2022-07-19},
  abstract = {Heart rate is under the precise control of the autonomic nervous system. However, the wiring of peripheral neural circuits\&nbsp;that regulate heart rate is poorly understood. Here, we develop a clearing-imaging-analysis pipeline to visualize innervation of intact hearts in 3D and employed a multi-technique approach to map parasympathetic and sympathetic neural circuits that control heart rate in mice. We identify cholinergic neurons and noradrenergic neurons in an intrinsic cardiac ganglion and the stellate ganglia, respectively, that project to the sinoatrial node. We also report that the heart rate response to optogenetic versus electrical stimulation of the vagus nerve displays different temporal characteristics and that vagal afferents enhance parasympathetic and reduce sympathetic tone to the heart via central mechanisms. Our findings provide new insights into neural regulation of heart rate, and our methodology to study cardiac circuits can be readily used to interrogate neural control of other visceral organs. The wiring of peripheral neural circuits that regulate heart rate is poorly understood. In this study, authors used tissue clearing for high-resolution characterization of nerves in the heart in 3D and transgenic and novel viral vector approaches to identify peripheral parasympathetic and sympathetic neuronal populations involved in heart rate control in mice.},
  keywords = {Motor neuron,Neural circuits,Peripheral nervous system},
  file = {/Users/asshah4/Library/CloudStorage/OneDrive-UniversityofIllinoisatChicago/articles/Rajendran et al_2019_Identification of peripheral neural circuits that regulate heart rate using.pdf}
}

@article{Rimoldi1990,
  title = {Analysis of Short-Term Oscillations of {{R-R}} and Arterial Pressure in Conscious Dogs.},
  author = {Rimoldi, O and Pierini, S and Ferrari, A and Cerutti, S and Pagani, M and Malliani, A},
  date = {1990-04},
  journaltitle = {The American journal of physiology},
  volume = {258},
  eprint = {2109943},
  eprinttype = {pmid},
  pages = {H967-76},
  issn = {0002-9513},
  doi = {10.1152/ajpheart.1990.258.4.H967},
  url = {http://www.ncbi.nlm.nih.gov/pubmed/2109943},
  urldate = {2018-09-29},
  abstract = {We studied the neural determinants of the second (i.e., high frequency, HF)- and third-order (i.e., low frequency, LF) spontaneous oscillations of heart period (R-R interval) and arterial pressure (AP) in conscious dogs, with the hypothesis that they might furnish quantitative markers of autonomic controlling activities. Spectral analysis of simultaneous R-R and AP variabilities quantified these oscillations that were also evaluated in units normalized by total power to focus on the balance of these two major components. At rest we observed a prevalent HF component (approximately 0.25 Hz) in R-R and AP variabilities that was synchronous with respiration. This HF component of R-R variability disappeared after atropine infusion and can be considered a marker mostly of vagal activity. When baroreceptor unloading, obtained by moderate hypotension, increased sympathetic activity the LF component increased in R-R, systolic, and diastolic AP variabilities. This increase in LF was not present after ganglionic blockade or after chronic arterial baroreceptor denervation. After chronic bilateral stellectomy, hypotension was not accompanied by an increase in LF component of R-R variability, while LF component remained in AP variability. An increase in LF component of R-R and AP variabilities was observed during transient coronary artery occlusion.},
  isbn = {0002-9513 (Print)\textbackslash r0002-9513 (Linking)},
  issue = {4 Pt 2},
  file = {/Users/asshah4/Library/CloudStorage/OneDrive-UniversityofIllinoisatChicago/articles/Rimoldi et al_1990_Analysis of short-term oscillations of R-R and arterial pressure in conscious.pdf}
}

@article{Robbe1987,
  title = {Assessment of Baroreceptor Reflex Sensitivity by Means of Spectral Analysis.},
  author = {Robbe, H W and Mulder, L J and Rüddel, H and Langewitz, W A and Veldman, J B and Mulder, G},
  date = {1987-11},
  journaltitle = {Hypertension (Dallas, Tex. : 1979)},
  volume = {10},
  number = {5},
  eprint = {3666866},
  eprinttype = {pmid},
  pages = {538--43},
  issn = {0194-911X},
  doi = {10.1161/01.hyp.10.5.538},
  url = {http://www.ncbi.nlm.nih.gov/pubmed/3666866},
  abstract = {A method of determining baroreceptor reflex sensitivity is proposed that is based on spectral analysis of systolic pressure values and RR interval times, namely, the modulus (or gain) in the mid frequency band (0.07-0.14 Hz) between these two signals. Results using this method were highly correlated (0.94; n = 8) with results of the phenylephrine method. In addition, compared with the values for the preceding rest period, the modulus decreased during mental challenge, as might be expected from the literature.},
  keywords = {baroreceptor reflex sensitivity,phenylephrine,spectral analysis},
  file = {/Users/asshah4/Library/CloudStorage/OneDrive-UniversityofIllinoisatChicago/articles/Robbe et al_1987_Assessment of baroreceptor reflex sensitivity by means of spectral analysis.pdf}
}

@article{Rumsfeld2016,
  title = {Big Data Analytics to Improve Cardiovascular Care: {{Promise}} and Challenges},
  author = {Rumsfeld, John S and Joynt, Karen E and Maddox, Thomas M},
  date = {2016},
  journaltitle = {Nature Reviews Cardiology},
  volume = {13},
  number = {6},
  eprint = {27009423},
  eprinttype = {pmid},
  pages = {350--359},
  issn = {17595010},
  doi = {10.1038/nrcardio.2016.42},
  url = {www.nature.com/nrcardio},
  urldate = {2020-04-17},
  abstract = {The potential for big data analytics to improve cardiovascular quality of care and patient outcomes is tremendous. However, the application of big data in health care is at a nascent stage, and the evidence to date demonstrating that big data analytics will improve care and outcomes is scant. This Review provides an overview of the data sources and methods that comprise big data analytics, and describes eight areas of application of big data analytics to improve cardiovascular care, including predictive modelling for risk and resource use, population management, drug and medical device safety surveillance, disease and treatment heterogeneity, precision medicine and clinical decision support, quality of care and performance measurement, and public health and research applications. We also delineate the important challenges for big data applications in cardiovascular care, including the need for evidence of effectiveness and safety, the methodological issues such as data quality and validation, and the critical importance of clinical integration and proof of clinical utility. If big data analytics are shown to improve quality of care and patient outcomes, and can be successfully implemented in cardiovascular practice, big data will fulfil its potential as an important component of a learning health-care system.},
  file = {/Users/asshah4/Library/CloudStorage/OneDrive-UniversityofIllinoisatChicago/articles/Rumsfeld et al_2016_Big data analytics to improve cardiovascular care.pdf}
}

@article{Saclova2022,
  title = {Reliable {{P}} Wave Detection in Pathological {{ECG}} Signals},
  author = {Saclova, Lucie and Nemcova, Andrea and Smisek, Radovan and Smital, Lukas and Vitek, Martin and Ronzhina, Marina},
  date = {2022-04-21},
  journaltitle = {Scientific Reports},
  shortjournal = {Sci Rep},
  volume = {12},
  number = {1},
  pages = {6589},
  publisher = {{Nature Publishing Group}},
  issn = {2045-2322},
  doi = {10.1038/s41598-022-10656-4},
  url = {https://www.nature.com/articles/s41598-022-10656-4},
  urldate = {2023-08-14},
  abstract = {Accurate automated detection of P waves in ECG allows to provide fast correct diagnosis of various cardiac arrhythmias and select suitable strategy for patients’ treatment. However, P waves detection is a still challenging task, especially in long-term ECGs with manifested cardiac pathologies. Software tools used in medical practice usually fail to detect P waves under pathological conditions. Most of recently published approaches have not been tested on such the signals at all. Here we introduce a novel method for accurate and reliable P wave detection, which is success in both normal and pathological cases. Our method uses phasor transform of ECG and innovative decision rules in order to improve P waves detection in pathological signals. The rules are based on a deep knowledge of heart manifestation during various arrhythmias, such as atrial fibrillation, premature ventricular contraction, etc. By involving the rules into the decision process, we are able to find the P wave in the correct location or, alternatively, not to search for it at all. In contrast to another studies, we use three, highly variable annotated ECG databases, which contain both normal and pathological records, to objectively validate our algorithm. The results for physiological records are Se\,=\,98.56\% and PP\,=\,99.82\% for MIT-BIH Arrhythmia Database (MITDP, with MITDB P-Wave Annotations) and Se\,=\,99.23\% and PP\,=\,99.12\% for QT database. These results are comparable with other published methods. For pathological signals, the proposed method reaches Se\,=\,96.40\% and PP\,=\,91.56\% for MITDB and Se\,=\,93.07\% and PP\,=\,88.60\% for Brno University of Technology ECG Signal Database with Annotations of P wave (BUT PDB). In these signals, the proposed detector greatly outperforms other methods and, thus, represents a huge step towards effective use of fully automated ECG analysis in a real medical practice.},
  issue = {1},
  langid = {english},
  keywords = {Biomedical engineering,Electrocardiography – EKG},
  file = {/Users/asshah4/Library/CloudStorage/OneDrive-UniversityofIllinoisatChicago/articles/Saclova et al_2022_Reliable P wave detection in pathological ECG signals.pdf}
}

@article{Saul1990,
  title = {Beat-{{To-Beat Variations}} of {{Heart Rate Reflect Modulation}} of {{Cardiac Autonomic Outflow}}},
  author = {family=Saul, given=JP, given-i=JP},
  date = {1990},
  journaltitle = {Physiology},
  volume = {5},
  number = {1},
  pages = {32--37},
  issn = {1548-9213},
  doi = {10.1152/physiologyonline.1990.5.1.32},
  url = {http://www.physiology.org/doi/10.1152/physiologyonline.1990.5.1.32},
  urldate = {2018-10-03},
  abstract = {JP Saul ABSTRACT What is most intriguing about heart rate (HR) variability is that there is so much of it. HR is constantly responding both rapidly and slowly to various physiological perturbations. We now understand that the frequency and amplitude of these HR fluctuations are indicative of the autonomic control systems underlying the response. Copyright © 1990 by International Union of Physiological Sciences},
  isbn = {1548-9213},
  file = {/Users/asshah4/Library/CloudStorage/OneDrive-UniversityofIllinoisatChicago/articles/Saul_1990_Beat-To-Beat Variations of Heart Rate Reflect Modulation of Cardiac Autonomic.pdf}
}

@article{Shah2016a,
  title = {An Electrocardiogram-Based Risk Equation for Incident Cardiovascular Disease from the {{National Health}} and {{Nutrition Examination Survey}}},
  author = {Shah, Amit J. and Vaccarino, Viola and Janssens, A. Cecile J.W. and Flanders, W. Dana and Kundu, Suman and Veledar, Emir and Wilson, Peter W.F. and Soliman, Elsayed Z.},
  date = {2016},
  journaltitle = {JAMA Cardiology},
  volume = {1},
  number = {7},
  pages = {779--786},
  issn = {23806591},
  doi = {10.1001/jamacardio.2016.2173},
  abstract = {IMPORTANCE: Electrocardiography (ECG) may detect subclinical cardiovascular disease (CVD) in asymptomatic individuals, but its role in assessing adverse events beyond traditional risk factors is not clear. Interval and vector data that are commonly available on modern ECGs may offer independent prognostic information that improves risk classification. OBJECTIVES: To derive and validate a CVD risk equation based on ECG metrics and to determine its incremental benefit in addition to the Framingham risk score (FRS). DESIGN, SETTING, AND PARTICIPANTS: This study included 3640 randomly selected community-based adults aged 40 to 74 years without known CVD from the First National Health and Nutrition Examination Survey (NHANES I) cohort (1971-1975) and 6329 from the NHANES III cohort (1988-1994). Participants were sampled from across the United States. A risk score to assess incident nonfatal and fatal CVD events was derived based on computer-generated ECG data, including frontal P, R, and T axes; heart rate; and PR, QRS, and QT intervals from NHANES I. The most prognostic variables, along with age and sex, were incorporated into the NHANES ECG risk equation. The equation was evaluated in the NHANES III cohort for an independent validation. Follow-up in the NHANES III cohort was completed on December 31, 2006. Data for this study were analyzed from August 11, 2015, to May 20, 2016. MAIN OUTCOMES AND MEASURES: The primary end point was CVD death. Secondary outcomes included 10-year ischemic heart disease and all-cause death. RESULTS: The final study sample included 9969 participants (4714 men [47.3\%]; 5255 women [52.7\%]; mean [SD] age, 55.3 [10.1] years) from both cohorts. Frontal T axis, heart rate, and heart rate–corrected QT interval were the most significant ECG factors in the NHANES I cohort. In the validation cohort (NHANES III), the equation provided for prognostic information for fatal CVD with a hazard ratio (HR) of 3.23 (95\% CI, 2.82-3.72); the C statistic was 0.79 (95\% CI, 0.76-0.81). When added to the FRS in Cox proportional hazards regression models, the categorical (1\%, 5\%, and 10\% cutoffs) net reclassification improvement was 24\%. When the FRS and ECG scores were combined in a single model, the C statistic improved by 0.04 (95\% CI, 0.02-0.06) to 0.80 (95\% CI, 0.77-0.82). Similar improvements were noted when the ECG score was added to the pooled cohort equation. When the equation for prognostic information about ischemic heart disease and all-cause death was evaluated, the results were similar. CONCLUSIONS AND RELEVANCE: An ECG risk score based on age, sex, heart rate, frontal T axis, and QT interval assesses the risk for CVD and compares favorably with the FRS alone in an independent cohort of asymptomatic individuals. Although the ECG risk equation is low cost, further research is needed to ascertain whether this additional step in risk stratification may improve prevention efforts and reduce CVD events.},
  file = {/Users/asshah4/Library/CloudStorage/OneDrive-UniversityofIllinoisatChicago/articles/Shah et al_2016_An electrocardiogram-based risk equation for incident cardiovascular disease.pdf}
}

@article{Silva2020,
  title = {Towards Better Heartbeat Segmentation with Deep Learning Classification},
  author = {Silva, Pedro and Luz, Eduardo and Silva, Guilherme and Moreira, Gladston and Wanner, Elizabeth and Vidal, Flavio and Menotti, David},
  date = {2020-11-26},
  journaltitle = {Scientific Reports},
  shortjournal = {Sci Rep},
  volume = {10},
  number = {1},
  pages = {20701},
  publisher = {{Nature Publishing Group}},
  issn = {2045-2322},
  doi = {10.1038/s41598-020-77745-0},
  url = {https://www.nature.com/articles/s41598-020-77745-0},
  urldate = {2023-08-02},
  abstract = {The confidence of medical equipment is intimately related to false alarms. The higher the number of false events occurs, the less truthful is the equipment. In this sense, reducing (or suppressing) false positive alarms is hugely desirable. In this work, we propose a feasible and real-time approach that works as a validation method for a heartbeat segmentation third-party algorithm. The approach is based on convolutional neural networks (CNNs), which may be embedded in dedicated hardware. Our proposal aims to detect the pattern of a single heartbeat and classifies them into two classes: a heartbeat and not a heartbeat. For this, a seven-layer convolution network is employed for both data representation and classification. We evaluate our approach in two well-settled databases in the literature on the raw heartbeat signal. The first database is a conventional on-the-person database called MIT-BIH, and the second is one less uncontrolled off-the-person type database known as CYBHi. To evaluate the feasibility and the performance of the proposed approach, we use as a baseline the Pam-Tompkins algorithm, which is a well-known method in the literature and still used in the industry. We compare the baseline against the proposed approach: a CNN model validating the heartbeats detected by a third-party algorithm. In this work, the third-party algorithm is the same as the baseline for comparison purposes. The results support the feasibility of our approach showing that our method can enhance the positive prediction of the Pan-Tompkins algorithm from \$\$97.84\textbackslash\%\$\$/\$\$90.28\textbackslash\%\$\$to \$\$100.00\textbackslash\%\$\$/\$\$96.77\textbackslash\%\$\$by slightly decreasing the sensitivity from \$\$95.79\textbackslash\%\$\$/\$\$96.95\textbackslash\%\$\$to \$\$92.98\textbackslash\%/\$\$\$\$95.71\textbackslash\%\$\$on the MIT-BIH/CYBHi databases.},
  issue = {1},
  langid = {english},
  keywords = {Computational models,Data acquisition,Data processing,Machine learning,Network topology},
  file = {/Users/asshah4/Library/CloudStorage/OneDrive-UniversityofIllinoisatChicago/articles/Silva et al_2020_Towards better heartbeat segmentation with deep learning classification.pdf}
}

@article{Siontis2021,
  title = {Artificial Intelligence-Enhanced Electrocardiography in Cardiovascular Disease Management},
  author = {Siontis, Konstantinos C. and Noseworthy, Peter A. and Attia, Zachi I. and Friedman, Paul A.},
  date = {2021-07},
  journaltitle = {Nature Reviews Cardiology},
  shortjournal = {Nat Rev Cardiol},
  volume = {18},
  number = {7},
  pages = {465--478},
  publisher = {{Nature Publishing Group}},
  issn = {1759-5010},
  doi = {10.1038/s41569-020-00503-2},
  url = {https://www.nature.com/articles/s41569-020-00503-2},
  urldate = {2023-07-26},
  abstract = {The application of artificial intelligence (AI) to the electrocardiogram (ECG), a ubiquitous and standardized test, is an example of the ongoing transformative effect of AI on cardiovascular medicine. Although the ECG has long offered valuable insights into cardiac and non-cardiac health and disease, its interpretation requires considerable human expertise. Advanced AI methods, such as deep-learning convolutional neural networks, have enabled rapid, human-like interpretation of the ECG, while signals and patterns largely unrecognizable to human interpreters can be detected by multilayer AI networks with precision, making the ECG a powerful, non-invasive biomarker. Large sets of digital ECGs linked to rich clinical data have been used to develop AI models for the detection of left ventricular dysfunction, silent (previously undocumented and asymptomatic) atrial fibrillation and hypertrophic cardiomyopathy, as well as the determination of a person’s age, sex and race, among other phenotypes. The clinical and population-level implications of AI-based ECG phenotyping continue to emerge, particularly with the rapid rise in the availability of mobile and wearable ECG technologies. In this Review, we summarize the current and future state of the AI-enhanced ECG in the detection of cardiovascular disease in at-risk populations, discuss its implications for clinical decision-making in patients with cardiovascular disease and critically appraise potential limitations and unknowns.},
  issue = {7},
  langid = {english},
  keywords = {Cardiovascular diseases,Diagnosis,Interventional cardiology,Medical imaging},
  file = {/Users/asshah4/Library/CloudStorage/OneDrive-UniversityofIllinoisatChicago/articles/Siontis et al_2021_Artificial intelligence-enhanced electrocardiography in cardiovascular disease.pdf}
}

@article{Steenland2004a,
  title = {A Practical Guide to Dose-Response Analyses and Risk Assessment in Occupational Epidemiology},
  author = {Steenland, Kyle and Deddens, James A},
  date = {2004},
  journaltitle = {Epidemiology},
  volume = {15},
  number = {1},
  eprint = {14712148},
  eprinttype = {pmid},
  pages = {63--70},
  issn = {10443983},
  doi = {10.1097/01.ede.0000100287.45004.e7},
  abstract = {Dose-response modeling in occupational epidemiology is usually motivated by questions of causal inference (eg, is there a monotonic increase of risk with increasing exposure?) or risk assessment (eg, how much excess risk exists at any given level of exposure?). We focus on several approaches to dose-response in occupational cohort studies. Categorical analyses are useful for detecting the shape of dose-response. However, they depend on the number and location of cutpoints and result in step functions rather than smooth curves. Restricted cubic splines and penalized splines are useful parametric techniques that provide smooth curves. Although splines can complement categorical analyses, they do not provide interpretable parameters. The shapes of these curves will depend on the degree of "smoothing" chosen by the analyst. We recommend combining categorical analyses and some type of smoother, with the goal of developing a reasonably simple parametric model. A simple parametric model should serve as the goal of dose-response analyses because (1) most "true" exposure response curves in nature may be reasonably simple, (2) a simple parametric model is easily communicated and used by others, and (3) a simple parametric model is the best tool for risk assessors and regulators seeking to estimate individual excess risks per unit of exposure. We discuss these issues and others, including whether the best model is always the one that fits the best, reasons to prefer a linear model for risk in the low-exposure region when conducting risk assessment, and common methods of calculating excess lifetime risk at a given exposure from epidemiologic results (eg, from rate ratios). Points are illustrated using data from a study of dioxin and cancer.},
  isbn = {1044-3983},
  file = {/Users/asshah4/Library/CloudStorage/OneDrive-UniversityofIllinoisatChicago/articles/Steenland_Deddens_2004_A practical guide to dose-response analyses and risk assessment in occupational.pdf}
}

@article{Taylor1998b,
  title = {Mechanisms Underlying Very-Low-Frequency {{RR-interval}} Oscillations in Humans},
  author = {Taylor, J. Andrew and Carr, Deborah L. and Myers, Christopher W. and Eckberg, Dwain L.},
  date = {1998-08-11},
  journaltitle = {Circulation},
  volume = {98},
  number = {6},
  pages = {547--555},
  publisher = {{Lippincott Williams and Wilkins}},
  issn = {00097322},
  doi = {10.1161/01.CIR.98.6.547},
  abstract = {Background-Survival of post-myocardial infarction patients is related inversely to their levels of very-low-frequency (0.003 to 0.03 Hz) RR- interval variability. The physiological basis for such oscillations is unclear. In our study, we used blocking drugs to evaluate potential contributions of sympathetic and vagal mechanisms and the renin-angiotensin- aldosterone system to very-low-frequency RR-interval variability in 10 young healthy subjects. Methods and Results-We recorded RR intervals and arterial pressures during three separate sessions, with the patient in supine and 40 degree upright tilt positions, during 20-minute frequency (0.25 Hz) and tidal volume-controlled breathing after intravenous injections: saline (control), atenolol (0.2 mg/kg, β-adrenergic blockade), atropine sulfate (0.04 mg/kg, parasympathetic blockade), atenolol and atropine (complete autonomic blockade), and enalaprilat (0.02 mg/kg, ACE blockade). We integrated fast Fourier transform RR-interval spectral power at very low (0.003 to 0.03 Hz), low (0.05 to 0.15 Hz), and respiratory (0.2 to 0.3 Hz) frequencies. β- Adrenergic blockade had no significant effect on very-low-or low-frequency RR-interval power but increased respiratory frequency power 2-fold. ACE blockade had no significant effect on low or respiratory frequency RR- interval power but modestly (\textasciitilde{} 21.\%) increased very-low-frequency power in the supine (but not upright tilt) position (P{$<$}0.05). The most profound effects were exerted by parasympathetic blockade: Atropine, given alone or with atenolol, abolished nearly all RR-interval variability and decreased very-low-frequency variability by 92\%. Conclusions-Although very-low- frequency heart period rhythms are influenced by the renin-angiotensin- aldosterone system, as low and respiratory frequency RR-interval rhythms, they depend primarily on the presence of parasympathetic outflow. Therefore the prognostic value of very-low-frequency heart period oscillations may derive from the fundamental importance of parasympathetic mechanisms in cardiovascular health.},
  keywords = {adrenergic,beta,Heart rate,Receptors,Renin,Vagus nerve},
  file = {/Users/asshah4/Library/CloudStorage/OneDrive-UniversityofIllinoisatChicago/articles/Taylor et al_1998_Mechanisms underlying very-low-frequency RR-interval oscillations in humans.pdf}
}

@article{Tereshchenko2011,
  title = {A New Electrocardiogram Marker to Identify Patients at Low Risk for Ventricular Tachyarrhythmias: {{Sum}} Magnitude of the Absolute {{QRST}} Integral},
  author = {Tereshchenko, Larisa G and Cheng, Alan and Fetics, Barry J and Butcher, Barbara and Marine, Joseph E and Spragg, David D and Sinha, Sunil and Dalal, Darshan and Calkins, Hugh and Tomaselli, Gordon F and Berger, Ronald D},
  date = {2011},
  journaltitle = {Journal of Electrocardiology},
  volume = {44},
  number = {2},
  pages = {208--216},
  issn = {00220736},
  doi = {10.1016/j.jelectrocard.2010.08.012},
  url = {www.jecgonline.com},
  urldate = {2020-05-19},
  abstract = {Objective: We proposed and tested a novel electrocardiogram marker of risk of ventricular arrhythmias (VAs). Methods: Digital orthogonal electrocardiograms were recorded at rest before implantable cardioverter-defibrillator (ICD) implantation in 508 participants of a primary prevention ICDs prospective cohort study (mean ± SD age, 60 ± 12 years; 377 male [74\%]). The sum magnitude of the absolute QRST integral in 3 orthogonal leads (SAI QRST) was calculated. A derivation cohort of 128 patients was used to define a cutoff; a validation cohort (n = 380) was used to test a predictive value. Results: During a mean follow-up of 18 months, 58 patients received appropriate ICD therapies. The SAI QRST was lower in patients with VA (105.2 ± 60.1 vs 138.4 ± 85.7 mV - ms, P = .002). In the Cox proportional hazards analysis, patients with SAI QRST not exceeding 145 mV - ms had about 4-fold higher risk of VA (hazard ratio, 3.6; 95\% confidence interval, 1.96-6.71; P {$<$} .0001) and a 6-fold higher risk of monomorphic ventricular tachycardia (hazard ratio, 6.58; 95\% confidence interval, 1.46-29.69; P = .014), whereas prediction of polymorphic ventricular tachycardia or ventricular fibrillation did not reach statistical significance. Conclusion: High SAI QRST is associated with low risk of sustained VA in patients with structural heart disease. © 2011 Elsevier Inc. All rights reserved.},
  file = {/Users/asshah4/Library/CloudStorage/OneDrive-UniversityofIllinoisatChicago/articles/Tereshchenko et al_2011_A new electrocardiogram marker to identify patients at low risk for ventricular.pdf}
}

@inproceedings{Tereshchenko2015b,
  title = {Frequency Content and Characteristics of Ventricular Conduction},
  booktitle = {Journal of {{Electrocardiology}}},
  author = {Tereshchenko, Larisa G and Josephson, Mark E},
  date = {2015},
  volume = {48},
  number = {6},
  pages = {933--937},
  issn = {15328430},
  doi = {10.1016/j.jelectrocard.2015.08.034},
  abstract = {The spectrum of frequencies producing the QRS complex has not been fully explored. In this manuscript we review previous studies of QRS frequency content, and discuss our novel method of the conjoint analysis of the ECG signal in six dimensions: in the domain of three space dimensions, in time domain, and in frequency domain. Orbital frequency of QRS loop is introduced as a six-dimensional characteristic of ventricular conduction, which helped to reveal inapparent ventricular conduction, and to characterize electrophysiological substrate. In this paper, we review our novel method in the historical context.},
  keywords = {Frequency content,Vectorcardiogram},
  file = {/Users/asshah4/Library/CloudStorage/OneDrive-UniversityofIllinoisatChicago/articles/Tereshchenko_Josephson_2015_Frequency content and characteristics of ventricular conduction.pdf}
}

@inproceedings{Tereshchenko2018a,
  title = {Global {{Electrical Heterogeneity}}: {{Mechanisms}} and {{Clinical Significance}}},
  booktitle = {Computing in {{Cardiology}}},
  author = {Tereshchenko, Larisa G.},
  date = {2018-09-01},
  volume = {2018-Septe},
  publisher = {{IEEE Computer Society}},
  issn = {2325887X},
  doi = {10.22489/CinC.2018.165},
  abstract = {This review summarizes recent findings and discusses a clinical significance of a vectorcardiographic (VCG) Global electrical heterogeneity (GEH). GEH concept is based on the concept of the spatial ventricular gradient (SVG), which is a global measure of the dispersion of total recovery time. We quantify GEH by measuring five features of the SVG vector (SVG magnitude, direction (azimuth and elevation), a scalar value, and spatial QRS-T angle) on orthogonal XYZ ECG. In analysis of more than 20,000 adults we showed that GEH is independently associated with sudden cardiac death (SCD) after adjustment for demographics, cardiovascular disease (time-updated incident non-fatal cardiovascular events [coronary heart disease, heart failure, stroke, atrial fibrillation, use of beta-blockers], and known risk factors [cholesterol, triglycerides, physical activity index, smoking, diabetes, obesity, hypertension, antihypertensive medications, creatinine, alcohol intake, left ventricular ejection fraction, and time-updated ECG metrics (heart rate, QTc, QRS duration, ECG-left ventricular hypertrophy, bundle branch block or interventricular conduction delay)]. This finding suggests that GEH represents an independent electrophysiological substrate of SCD.},
  isbn = {978-1-72810-958-9},
  file = {/Users/asshah4/Library/CloudStorage/OneDrive-UniversityofIllinoisatChicago/articles/Tereshchenko_2018_Global Electrical Heterogeneity.pdf}
}

@article{Tervo2022,
  title = {Closed-Loop Optimization of Transcranial Magnetic Stimulation with Electroencephalography Feedback},
  author = {Tervo, Aino E. and Nieminen, Jaakko O. and Lioumis, Pantelis and Metsomaa, Johanna and Souza, Victor H. and Sinisalo, Heikki and Stenroos, Matti and Sarvas, Jukka and Ilmoniemi, Risto J.},
  date = {2022},
  journaltitle = {Brain Stimulation},
  shortjournal = {Brain Stimul},
  volume = {15},
  number = {2},
  eprint = {35337598},
  eprinttype = {pmid},
  pages = {523--531},
  issn = {1876-4754},
  doi = {10.1016/j.brs.2022.01.016},
  abstract = {BACKGROUND: Transcranial magnetic stimulation (TMS) is widely used in brain research and treatment of various brain dysfunctions. However, the optimal way to target stimulation and administer TMS therapies, for example, where and in which electric field direction the stimuli should be given, is yet to be determined. OBJECTIVE: To develop an automated closed-loop system for adjusting TMS parameters (in this work, the stimulus orientation) online based on TMS-evoked brain activity measured with electroencephalography (EEG). METHODS: We developed an automated closed-loop TMS-EEG set-up. In this set-up, the stimulus parameters are electronically adjusted with multi-locus TMS. As a proof of concept, we developed an algorithm that automatically optimizes the stimulation orientation based on single-trial EEG responses. We applied the algorithm to determine the electric field orientation that maximizes the amplitude of the TMS-EEG responses. The validation of the algorithm was performed with six healthy volunteers, repeating the search twenty times for each subject. RESULTS: The validation demonstrated that the closed-loop control worked as desired despite the large variation in the single-trial EEG responses. We were often able to get close to the orientation that maximizes the EEG amplitude with only a few tens of pulses. CONCLUSION: Optimizing stimulation with EEG feedback in a closed-loop manner is feasible and enables effective coupling to brain activity.},
  langid = {english},
  pmcid = {PMC8940636},
  keywords = {Bayesian optimization,Brain,Brain Mapping,Closed-loop,Electroencephalography,Feedback,Humans,Multi-channel TMS,Multi-locus TMS,Transcranial magnetic stimulation,Transcranial Magnetic Stimulation},
  file = {/Users/asshah4/Library/CloudStorage/OneDrive-UniversityofIllinoisatChicago/articles/Tervo et al_2022_Closed-loop optimization of transcranial magnetic stimulation with.pdf}
}

@article{Thenmozhi2019,
  title = {Survival Analysis in Longitudinal Studies for Recurrent Events: {{Applications}} and Challenges},
  shorttitle = {Survival Analysis in Longitudinal Studies for Recurrent Events},
  author = {Thenmozhi, Mani and Jeyaseelan, Visalakshi and Jeyaseelan, Lakshmanan and Isaac, Rita and Vedantam, Rupa},
  date = {2019-06-01},
  journaltitle = {Clinical Epidemiology and Global Health},
  shortjournal = {Clinical Epidemiology and Global Health},
  volume = {7},
  number = {2},
  pages = {253--260},
  publisher = {{Elsevier}},
  issn = {2213-3984},
  doi = {10.1016/j.cegh.2019.01.013},
  url = {https://cegh.net/article/S2213-3984%2818%2930218-5/fulltext},
  urldate = {2023-07-24},
  langid = {english},
  keywords = {Data structure,Extended Cox model,Frailty model,Recurrent events,Upper respiratory infection},
  file = {/Users/asshah4/Library/CloudStorage/OneDrive-UniversityofIllinoisatChicago/articles/Thenmozhi et al_2019_Survival analysis in longitudinal studies for recurrent events.pdf}
}

@article{Tison2019b,
  title = {Automated and {{Interpretable Patient ECG Profiles}} for {{Disease Detection}}, {{Tracking}}, and {{Discovery}}},
  author = {Tison, Geoffrey H. and Zhang, Jeffrey and Delling, Francesca N. and Deo, Rahul C.},
  date = {2019-09-01},
  journaltitle = {Circulation. Cardiovascular quality and outcomes},
  volume = {12},
  number = {9},
  eprint = {1807.02569},
  eprinttype = {arxiv},
  pages = {e005289},
  publisher = {{NLM (Medline)}},
  issn = {19417705},
  doi = {10.1161/CIRCOUTCOMES.118.005289},
  abstract = {BACKGROUND: The ECG remains the most widely used diagnostic test for characterization of cardiac structure and electrical activity. We hypothesized that parallel advances in computing power, machine learning algorithms, and availability of large-scale data could substantially expand the clinical inferences derived from the ECG while at the same time preserving interpretability for medical decision-making. METHODS AND RESULTS: We identified 36 186 ECGs from the University of California, San Francisco database that would enable training of models for estimation of cardiac structure or function or detection of disease. We segmented the ECG into standard component waveforms and intervals using a novel combination of convolutional neural networks and hidden Markov models and evaluated this segmentation by comparing resulting electrical intervals against 141 864 measurements produced during the clinical workflow. We then built a patient-level ECG profile, a 725-element feature vector and used this profile to train and interpret machine learning models for examples of cardiac structure (left ventricular mass, left atrial volume, and mitral annulus e-prime) and disease (pulmonary arterial hypertension, hypertrophic cardiomyopathy, cardiac amyloid, and mitral valve prolapse). ECG measurements derived from the convolutional neural network-hidden Markov model segmentation agreed with clinical estimates, with median absolute deviations as a fraction of observed value of 0.6\% for heart rate and 4\% for QT interval. Models trained using patient-level ECG profiles enabled surprising quantitative estimates of left ventricular mass and mitral annulus e' velocity (median absolute deviation of 16\% and 19\%, respectively) with good discrimination for left ventricular hypertrophy and diastolic dysfunction as binary traits. Model performance using our approach for disease detection demonstrated areas under the receiver operating characteristic curve of 0.94 for pulmonary arterial hypertension, 0.91 for hypertrophic cardiomyopathy, 0.86 for cardiac amyloid, and 0.77 for mitral valve prolapse. CONCLUSIONS: Modern machine learning methods can extend the 12-lead ECG to quantitative applications well beyond its current uses while preserving the transparency that is so fundamental to clinical care.},
  keywords = {heart rate,hypertension,machine learning,mitral valve prolapse,work flow},
  file = {/Users/asshah4/Library/CloudStorage/OneDrive-UniversityofIllinoisatChicago/articles/Tison et al_2019_Automated and Interpretable Patient ECG Profiles for Disease Detection,.pdf}
}

@article{Tutuko2022,
  title = {{{DAE-ConvBiLSTM}}: {{End-to-end}} Learning Single-Lead Electrocardiogram Signal for Heart Abnormalities Detection},
  shorttitle = {{{DAE-ConvBiLSTM}}},
  author = {Tutuko, Bambang and Darmawahyuni, Annisa and Nurmaini, Siti and Tondas, Alexander Edo and Naufal Rachmatullah, Muhammad and Teguh, Samuel Benedict Putra and Firdaus, Firdaus and Sapitri, Ade Iriani and Passarella, Rossi},
  date = {2022-12-30},
  journaltitle = {PLOS ONE},
  shortjournal = {PLoS One},
  volume = {17},
  number = {12},
  eprint = {36584187},
  eprinttype = {pmid},
  pages = {e0277932},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0277932},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9803308/},
  urldate = {2023-07-31},
  abstract = {Background The electrocardiogram (ECG) is a widely used diagnostic that observes the heart activities of patients to ascertain a heart abnormality diagnosis. The artifacts or noises are primarily associated with the problem of ECG signal processing. Conventional denoising techniques have been proposed in previous literature; however, some lacks, such as the determination of suitable wavelet basis function and threshold, can be a time-consuming process. This paper presents end-to-end learning using a denoising auto-encoder (DAE) for denoising algorithms and convolutional-bidirectional long short-term memory (ConvBiLSTM) for ECG delineation to classify ECG waveforms in terms of the PQRST-wave and isoelectric lines. The denoising reconstruction using unsupervised learning based on the encoder-decoder process can be proposed to improve the drawbacks. First, The ECG signals are reduced to a low-dimensional vector in the encoder. Second, the decoder reconstructed the signals. The last, the reconstructed signals of ECG can be processed to ConvBiLSTM. The proposed architecture of DAE-ConvBiLSTM is the end-to-end diagnosis of heart abnormality detection. Results As a result, the performance of DAE-ConvBiLSTM has obtained an average of above 98.59\% accuracy, sensitivity, specificity, precision, and F1 score from the existing studies. The DAE-ConvBiLSTM has also experimented with detecting T-wave (due to ventricular repolarisation) morphology abnormalities. Conclusion The development architecture for detecting heart abnormalities using an unsupervised learning DAE and supervised learning ConvBiLSTM can be proposed for an end-to-end learning algorithm. In the future, the precise accuracy of the ECG main waveform will affect heart abnormalities detection in clinical practice.},
  pmcid = {PMC9803308},
  file = {/Users/asshah4/Library/CloudStorage/OneDrive-UniversityofIllinoisatChicago/articles/Tutuko et al_2022_DAE-ConvBiLSTM.pdf}
}

@article{Uno2013,
  title = {A Unified Inference Procedure for a Class of Measures to Assess Improvement in Risk Prediction Systems with Survival Data},
  author = {Uno, Hajime and Tian, Lu and Cai, Tianxi and Kohane, Isaac S. and Wei, L. J.},
  date = {2013-06-30},
  journaltitle = {Statistics in Medicine},
  volume = {32},
  number = {14},
  eprint = {23037800},
  eprinttype = {pmid},
  pages = {2430--2442},
  issn = {02776715},
  doi = {10.1002/sim.5647},
  abstract = {Risk prediction procedures can be quite useful for the patient's treatment selection, prevention strategy, or disease management in evidence-based medicine. Often, potentially important new predictors are available in addition to the conventional markers. The question is how to quantify the improvement from the new markers for prediction of the patient's risk in order to aid cost-benefit decisions. The standard method, using the area under the receiver operating characteristic curve, to measure the added value may not be sensitive enough to capture incremental improvements from the new markers. Recently, some novel alternatives to area under the receiver operating characteristic curve, such as integrated discrimination improvement and net reclassification improvement, were proposed. In this paper, we consider a class of measures for evaluating the incremental values of new markers, which includes the preceding two as special cases. We present a unified procedure for making inferences about measures in the class with censored event time data. The large sample properties of our procedures are theoretically justified. We illustrate the new proposal with data from a cancer study to evaluate a new gene score for prediction of the patient's survival. © 2012 John Wiley \& Sons, Ltd.},
  keywords = {Area under the receiver operating characteristic curve,C-statistic,Cox's regression,Integrated discrimination improvement,Net reclassification improvement,Risk prediction},
  file = {/Users/asshah4/Library/CloudStorage/OneDrive-UniversityofIllinoisatChicago/articles/Uno et al_2013_A unified inference procedure for a class of measures to assess improvement in.pdf}
}

@article{vandeLeur2021,
  title = {Discovering and {{Visualizing Disease-Specific Electrocardiogram Features Using Deep Learning}}},
  author = {family=Leur, given=Rutger R., prefix=van de, useprefix=true and Taha, Karim and Bos, Max N. and family=Heijden, given=Jeroen F., prefix=van der, useprefix=true and Gupta, Deepak and Cramer, Maarten J. and Hassink, Rutger J. and family=Harst, given=Pim, prefix=van der, useprefix=true and Doevendans, Pieter A. and Asselbergs, Folkert W. and family=Es, given=René, prefix=van, useprefix=true},
  date = {2021-02},
  journaltitle = {Circulation: Arrhythmia and Electrophysiology},
  volume = {14},
  number = {2},
  pages = {e009056},
  publisher = {{American Heart Association}},
  doi = {10.1161/CIRCEP.120.009056},
  url = {https://www.ahajournals.org/doi/full/10.1161/CIRCEP.120.009056},
  urldate = {2023-07-27},
  abstract = {Download figureDownload PowerPoint},
  keywords = {arrhythmogenic right ventricular dysplasia,cardiomyopathies,deep learning,mutation},
  file = {/Users/asshah4/Library/CloudStorage/OneDrive-UniversityofIllinoisatChicago/articles/van de Leur et al_2021_Discovering and Visualizing Disease-Specific Electrocardiogram Features SUPPLEMENT.pdf;/Users/asshah4/Library/CloudStorage/OneDrive-UniversityofIllinoisatChicago/articles/van de Leur et al_2021_Discovering and Visualizing Disease-Specific Electrocardiogram Features Using.pdf}
}

@article{VanderWeele2007,
  title = {Four Types of Effect Modification: {{A}} Classification Based on Directed Acyclic Graphs},
  author = {VanderWeele, Tyler J. and Robins, James M.},
  date = {2007-09},
  journaltitle = {Epidemiology},
  volume = {18},
  number = {5},
  eprint = {17700242},
  eprinttype = {pmid},
  pages = {561--568},
  issn = {10443983},
  doi = {10.1097/EDE.0b013e318127181b},
  abstract = {It is possible to classify the types of causal relationships that can give rise to effect modification on the risk difference scale by expressing the conditional causal risk-difference as a sum of products of stratum-specific risk differences and conditional probabilities. Directed acyclic graphs clarify the causal relationships necessary for a particular variable to serve as an effect modifier for the causal risk difference involving 2 other variables. The directed acyclic graph causal framework thereby gives rise to a 4-fold classification for effect modification: direct effect modification, indirect effect modification, effect modification by proxy and effect modification by a common cause. We briefly discuss the case of multiple effect modification relationships and multiple effect modifiers as well as measures of effect other than that of the causal risk difference. © 2007 Lippincott Williams \& Wilkins, Inc.},
  file = {/Users/asshah4/Library/CloudStorage/OneDrive-UniversityofIllinoisatChicago/articles/VanderWeele_Robins_2007_Four types of effect modification.pdf}
}

@article{Verweij2020,
  title = {The {{Genetic Makeup}} of the {{Electrocardiogram}}},
  author = {Verweij, Niek and Benjamins, Jan-Walter and Morley, Michael P. and Van De Vegte, Yordi J. and Teumer, Alexander and Trenkwalder, Teresa and Reinhard, Wibke and Cappola, Thomas P. and Van Der Harst, Pim},
  date = {2020-09},
  journaltitle = {Cell Systems},
  shortjournal = {Cell Systems},
  volume = {11},
  number = {3},
  pages = {229-238.e5},
  issn = {24054712},
  doi = {10.1016/j.cels.2020.08.005},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S2405471220302891},
  urldate = {2023-06-09},
  abstract = {The electrocardiogram (ECG) is one of the most useful non-invasive diagnostic tests for a wide array of cardiac disorders. Traditional approaches to analyzing ECGs focus on individual segments. Here, we performed comprehensive deep phenotyping of 77,190 ECGs in the UK Biobank across the complete cycle of cardiac conduction, resulting in 500 spatial-temporal datapoints, across 10 million genetic variants. In addition to characterizing polygenic risk scores for the traditional ECG segments, we identified over 300 genetic loci that are statistically associated with the high-dimensional representation of the ECG. We established the genetic ECG signature for dilated cardiomyopathy, associated the BAG3, HSPB7/CLCNKA, PRKCA, TMEM43, and OBSCN loci with disease risk and confirmed this association in an independent cohort. In total, our work demonstrates that a high-dimensional analysis of the entire ECG provides unique opportunities for studying cardiac biology and disease and furthering drug development.},
  langid = {english},
  file = {/Users/asshah4/Library/CloudStorage/OneDrive-UniversityofIllinoisatChicago/articles/Verweij et al_2020_The Genetic Makeup of the Electrocardiogram.pdf}
}

@article{Vitolo2021,
  title = {Clinical {{Phenotype Classification}} of {{Atrial Fibrillation Patients Using Cluster Analysis}} and {{Associations}} with {{Trial-Adjudicated Outcomes}}},
  author = {Vitolo, Marco and Proietti, Marco and Shantsila, Alena and Boriani, Giuseppe and Lip, Gregory Y. H.},
  date = {2021-07-20},
  journaltitle = {Biomedicines},
  volume = {9},
  number = {7},
  pages = {843},
  issn = {2227-9059},
  doi = {10.3390/biomedicines9070843},
  abstract = {{$<$}p{$>$}Background and purpose: Given the great clinical heterogeneity of atrial fibrillation (AF) patients, conventional classification only based on disease subtype or arrhythmia patterns may not adequately characterize this population. We aimed to identify different groups of AF patients who shared common clinical phenotypes using cluster analysis and evaluate the association between identified clusters and clinical outcomes. Methods: We performed a hierarchical cluster analysis in AF patients from AMADEUS and BOREALIS trials. The primary outcome was a composite of stroke/thromboembolism (TE), cardiovascular (CV) death, myocardial infarction, and/or all-cause death. Individual components of the primary outcome and major bleeding were also assessed. Results: We included 3980 AF patients treated with the Vitamin-K Antagonist from the AMADEUS and BOREALIS studies. The analysis identified four clusters in which patients varied significantly among clinical characteristics. Cluster 1 was characterized by patients with low rates of CV risk factors and comorbidities; Cluster 2 was characterized by patients with a high burden of CV risk factors; Cluster 3 consisted of patients with a high burden of CV comorbidities; Cluster 4 was characterized by the highest rates of non-CV comorbidities. After a mean follow-up of 365 (standard deviation 187) days, Cluster 4 had the highest cumulative risk of outcomes. Compared with Cluster 1, Cluster 4 was independently associated with an increased risk for the composite outcome (hazard ratio (HR) 2.43, 95\% confidence interval (CI) 1.70–3.46), all-cause death (HR 2.35, 95\% CI 1.58–3.49) and major bleeding (HR 2.18, 95\% CI 1.19–3.96). Conclusions: Cluster analysis identified four different clinically relevant phenotypes of AF patients that had unique clinical characteristics and different outcomes. Cluster analysis highlights the high degree of heterogeneity in patients with AF, suggesting the need for a phenotype-driven approach to comorbidities, which could provide a more holistic approach to management aimed to improve patients’ outcomes.{$<$}/p{$>$}}
}

@article{Waks2016,
  title = {Global Electrical Heterogeneity: {{A}} Review of the Spatial Ventricular Gradient},
  author = {Waks, Jonathan W and Tereshchenko, Larisa G},
  date = {2016},
  journaltitle = {Journal of Electrocardiology},
  volume = {49},
  number = {6},
  pages = {824--830},
  issn = {15328430},
  doi = {10.1016/j.jelectrocard.2016.07.025},
  url = {www.sciencedirect.comhttp://dx.doi.org/10.1016/j.jelectrocard.2016.07.0250022-0736/},
  urldate = {2020-03-30},
  abstract = {The ventricular gradient, an electrocardiographic concept calculated by integrating the area under the QRS complex and T-wave, represents the degree and direction of myocardial electrical heterogeneity. Although the concept of the ventricular gradient was first introduced in the 1930s, it has not yet found a place in routine electrocardiography. In the modern era, it is relatively simple to calculate the ventricular gradient in three dimensions (the spatial ventricular gradient (SVG)), and there is now renewed interest in using the SVG as a tool for risk stratification of ventricular arrhythmias and sudden cardiac death. This manuscript will review the history of the ventricular gradient, describe its electrophysiological meaning and significance, and discuss its clinical utility.},
  keywords = {Spatial ventricular gradient,Vectorcardiogram},
  file = {/Users/asshah4/Library/CloudStorage/OneDrive-UniversityofIllinoisatChicago/articles/Waks_Tereshchenko_2016_Global electrical heterogeneity.pdf}
}

@article{Waks2016a,
  title = {Global {{Electric Heterogeneity Risk Score}} for {{Prediction}} of {{Sudden Cardiac Death}} in the {{General Population}}},
  author = {Waks, Jonathan W. and Sitlani, Colleen M. and Soliman, Elsayed Z. and Kabir, Muammar and Ghafoori, Elyar and Biggs, Mary L. and Henrikson, Charles A. and Sotoodehnia, Nona and Biering-Sørensen, Tor and Agarwal, Sunil K. and Siscovick, David S. and Post, Wendy S. and Solomon, Scott D. and Buxton, Alfred E. and Josephson, Mark E. and Tereshchenko, Larisa G.},
  date = {2016-06-07},
  journaltitle = {Circulation},
  volume = {133},
  number = {23},
  eprint = {27081116},
  eprinttype = {pmid},
  pages = {2222--2234},
  publisher = {{Lippincott Williams and Wilkins}},
  issn = {0009-7322},
  doi = {10.1161/CIRCULATIONAHA.116.021306},
  url = {https://www.ahajournals.org/doi/10.1161/CIRCULATIONAHA.116.021306},
  urldate = {2020-05-19},
  abstract = {Background - Asymptomatic individuals account for the majority of sudden cardiac deaths (SCDs). Development of effective, low-cost, and noninvasive SCD risk stratification tools is necessary. Methods and Results - Participants from the Atherosclerosis Risk in Communities study and Cardiovascular Health Study (n=20 177; age, 59.3±10.1 years; age range, 44-100 years; 56\% female; 77\% white) were followed up for 14.0 years (median). Five ECG markers of global electric heterogeneity (GEH; sum absolute QRST integral, spatial QRST angle, spatial ventricular gradient [SVG] magnitude, SVG elevation, and SVG azimuth) were measured on standard 12-lead ECGs. Cox proportional hazards and competing risks models evaluated associations between GEH electrocardiographic parameters and SCD. An SCD competing risks score was derived from demographics, comorbidities, and GEH parameters. SCD incidence was 1.86 per 1000 person-years. After multivariable adjustment, baseline GEH parameters and large increases in GEH parameters over time were independently associated with SCD. Final SCD risk scores included age, sex, race, diabetes mellitus, hypertension, coronary heart disease, stroke, and GEH parameters as continuous variables. When GEH parameters were added to clinical/demographic factors, the C statistic increased from 0.777 to 0.790 (P=0.008), the risk score classified 10-year SCD risk as high ({$>$}5\%) in 7.2\% of participants, 10\% of SCD victims were appropriately reclassified into a high-risk category, and only 1.4\% of SCD victims were inappropriately reclassified from high to intermediate risk. The net reclassification index was 18.3\%. Conclusions - Abnormal electrophysiological substrate quantified by GEH parameters is independently associated with SCD in the general population. The addition of GEH parameters to clinical characteristics improves SCD risk prediction.},
  keywords = {cardiac,death,electrocardiography,electrophysiology,risk assessment,sudden},
  file = {/Users/asshah4/Library/CloudStorage/OneDrive-UniversityofIllinoisatChicago/articles/Waks et al_2016_Global Electric Heterogeneity Risk Score for Prediction of Sudden Cardiac Death.pdf}
}

@article{Watanabe2021,
  title = {Clinical Phenotypes of Patients with Non-Valvular Atrial Fibrillation as Defined by a Cluster Analysis: {{A}} Report from the {{J-RHYTHM}} Registry},
  author = {Watanabe, Eiichi and Inoue, Hiroshi and Atarashi, Hirotsugu and Okumura, Ken and Yamashita, Takeshi and Kodani, Eitaro and Kiyono, Ken and Origasa, Hideki},
  date = {2021-12},
  journaltitle = {IJC Heart \& Vasculature},
  volume = {37},
  pages = {100885},
  issn = {23529067},
  doi = {10.1016/j.ijcha.2021.100885}
}

@article{Williams2021,
  title = {{{OpenEP}}: {{A Cross-Platform Electroanatomic Mapping Data Format}} and {{Analysis Platform}} for {{Electrophysiology Research}}},
  author = {Williams, Steven E. and Roney, Caroline H. and Connolly, Adam and Sim, Iain and Whitaker, John and O’Hare, Daniel and Kotadia, Irum and O’Neill, Louisa and Corrado, Cesare and Bishop, Martin and Niederer, Steven A. and Wright, Matt and O’Neill, Mark and Linton, Nick W.F.},
  date = {2021-02-26},
  journaltitle = {Frontiers in Physiology},
  volume = {12},
  pages = {160},
  publisher = {{Frontiers Media S.A.}},
  issn = {1664042X},
  doi = {10.3389/FPHYS.2021.646023/BIBTEX},
  abstract = {Background: Electroanatomic mapping systems are used to support electrophysiology research. Data exported from these systems is stored in proprietary formats which are challenging to access and storage-space inefficient. No previous work has made available an open-source platform for parsing and interrogating this data in a standardized format. We therefore sought to develop a standardized, open-source data structure and associated computer code to store electroanatomic mapping data in a space-efficient and easily accessible manner. Methods: A data structure was defined capturing the available anatomic and electrical data. OpenEP, implemented in MATLAB, was developed to parse and interrogate this data. Functions are provided for analysis of chamber geometry, activation mapping, conduction velocity mapping, voltage mapping, ablation sites, and electrograms as well as visualization and input/output functions. Performance benchmarking for data import and storage was performed. Data import and analysis validation was performed for chamber geometry, activation mapping, voltage mapping and ablation representation. Finally, systematic analysis of electrophysiology literature was performed to determine the suitability of OpenEP for contemporary electrophysiology research. Results: The average time to parse clinical datasets was 400 ± 162 s per patient. OpenEP data was two orders of magnitude smaller than compressed clinical data (OpenEP: 20.5 ± 8.7 Mb, vs clinical: 1.46 ± 0.77 Gb). OpenEP-derived geometry metrics were correlated with the same clinical metrics (Area: R2 = 0.7726, P {$<$} 0.0001; Volume: R2 = 0.5179, P {$<$} 0.0001). Investigating the cause of systematic bias in these correlations revealed OpenEP to outperform the clinical platform in recovering accurate values. Both activation and voltage mapping data created with OpenEP were correlated with clinical values (mean voltage R2 = 0.8708, P {$<$} 0.001; local activation time R2 = 0.8892, P {$<$} 0.0001). OpenEP provides the processing necessary for 87 of 92 qualitatively assessed analysis techniques (95\%) and 119 of 136 quantitatively assessed analysis techniques (88\%) in a contemporary cohort of mapping studies. Conclusions: We present the OpenEP framework for evaluating electroanatomic mapping data. OpenEP provides the core functionality necessary to conduct electroanatomic mapping research. We demonstrate that OpenEP is both space-efficient and accurately representative of the original data. We show that OpenEP captures the majority of data required for contemporary electroanatomic mapping-based electrophysiology research and propose a roadmap for future development.},
  keywords = {ablation electrophysiology,atrial fibrillation,conduction velocity,contact force,data storage and retrieval,electroanatomic mapping,electrophysiology – arrhythmia mapping and ablatio},
  file = {/Users/asshah4/Library/CloudStorage/OneDrive-UniversityofIllinoisatChicago/articles/Williams et al_2021_OpenEP.pdf}
}

@article{Wu2022,
  title = {Polygenic Power Calculator: {{Statistical}} Power and Polygenic Prediction Accuracy of Genome-Wide Association Studies of Complex Traits},
  shorttitle = {Polygenic Power Calculator},
  author = {Wu, Tian and Liu, Zipeng and Mak, Timothy Shin Heng and Sham, Pak Chung},
  date = {2022-10-10},
  journaltitle = {Frontiers in Genetics},
  shortjournal = {Front Genet},
  volume = {13},
  eprint = {36299579},
  eprinttype = {pmid},
  pages = {989639},
  issn = {1664-8021},
  doi = {10.3389/fgene.2022.989639},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9589038/},
  urldate = {2023-10-12},
  abstract = {Power calculation is a necessary step when planning genome-wide association studies (GWAS) to ensure meaningful findings. Statistical power of GWAS depends on the genetic architecture of phenotype, sample size, and study design. While several computer programs have been developed to perform power calculation for single SNP association testing, it might be more appropriate for GWAS power calculation to address the probability of detecting any number of associated SNPs. In this paper, we derive the statistical power distribution across causal SNPs under the assumption of a point-normal effect size distribution. We demonstrate how key outcome indices of GWAS are related to the genetic architecture (heritability and polygenicity) of the phenotype through the power distribution. We also provide a fast, flexible and interactive power calculation tool which generates predictions for key GWAS outcomes including the number of independent significant SNPs, the phenotypic variance explained by these SNPs, and the predictive accuracy of resulting polygenic scores. These results could also be used to explore the future behaviour of GWAS as sample sizes increase further. Moreover, we present results from simulation studies to validate our derivation and evaluate the agreement between our predictions and reported GWAS results.},
  pmcid = {PMC9589038},
  file = {/Users/asshah4/Library/CloudStorage/OneDrive-UniversityofIllinoisatChicago/articles/Wu et al_2022_Polygenic power calculator.pdf}
}

@article{Yaniv2014,
  title = {Synchronization of Sinoatrial Node Pacemaker Cell Clocks and Its Autonomic Modulation Impart Complexity to Heart Beating Intervals},
  author = {Yaniv, Yael and Ahmet, Ismayil and Liu, Jie and Lyashkov, Alexey E. and Guiriba, Toni Rose and Okamoto, Yosuke and Ziman, Bruce D. and Lakatta, Edward G.},
  date = {2014},
  journaltitle = {Heart Rhythm},
  volume = {11},
  number = {7},
  pages = {1210--1219},
  publisher = {{Elsevier}},
  issn = {15563871},
  doi = {10.1016/j.hrthm.2014.03.049},
  url = {http://dx.doi.org/10.1016/j.hrthm.2014.03.049},
  abstract = {Background A reduction of complexity of heart beating interval variability that is associated with an increased morbidity and mortality in cardiovascular disease states is thought to derive from the balance of sympathetic and parasympathetic neural impulses to the heart. However, rhythmic clocklike behavior intrinsic to pacemaker cells in the sinoatrial node (SAN) drives their beating, even in the absence of autonomic neural input. Objective To test how this rhythmic clocklike behavior intrinsic to pacemaker cells interacts with autonomic impulses to the heart beating interval variability in vivo. Methods We analyzed beating interval variability in time and frequency domains and by fractal and entropy analyses: (1) in vivo, when the brain input to the SAN is intact; (2) during autonomic denervation in vivo; (3) in isolated SAN tissue (ie, in which the autonomic neural input is completely absent); (4) in single pacemaker cells isolated from the SAN; and (5) after autonomic receptor stimulation of these cells. Results Spontaneous beating intervals of pacemaker cells residing in the isolated SAN tissue exhibit fractal-like behavior and have lower approximate entropy compared with those in the intact heart. Isolation of pacemaker cells from SAN tissue, however, leads to a loss in the beating interval order and fractal-like behavior. β-Adrenergic receptor stimulation of isolated pacemaker cells increases intrinsic clock synchronization, decreases their action potential period, and increases system complexity. Conclusions Both the average beating interval in vivo and beating interval complexity are conferred by the combined effects of clock periodicity intrinsic to pacemaker cells and their response to autonomic neural input. © 2014 Heart Rhythm Society.},
  keywords = {Autonomic neural impulse,chaotic processes,Fractal-like behavior,Heart rate variability,sinoatrial node pacemaker cell},
  file = {/Users/asshah4/Library/CloudStorage/OneDrive-UniversityofIllinoisatChicago/articles/Yaniv et al_2014_Synchronization of sinoatrial node pacemaker cell clocks and its autonomic.pdf}
}
